{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"authorship_tag":"ABX9TyMbKGTK525va4gtoTXtzK31","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Probablistic retrival model, Fundamental of RAG","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:31:56.731885Z","iopub.execute_input":"2025-12-24T07:31:56.732262Z","iopub.status.idle":"2025-12-24T07:31:56.736991Z","shell.execute_reply.started":"2025-12-24T07:31:56.732230Z","shell.execute_reply":"2025-12-24T07:31:56.736097Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Initialize the tokenizer and the model\nmodel_id = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:31:56.751286Z","iopub.execute_input":"2025-12-24T07:31:56.751604Z","iopub.status.idle":"2025-12-24T07:31:57.405415Z","shell.execute_reply.started":"2025-12-24T07:31:56.751572Z","shell.execute_reply":"2025-12-24T07:31:57.404651Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print(tokenizer.encode(\"<|endoftext|>\",return_tensors=\"pt\"))\nprint(tokenizer.decode(range(200)))\nprint(tokenizer.decode([20755]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:40:18.685798Z","iopub.execute_input":"2025-12-24T07:40:18.686128Z","iopub.status.idle":"2025-12-24T07:40:18.692402Z","shell.execute_reply.started":"2025-12-24T07:40:18.686098Z","shell.execute_reply":"2025-12-24T07:40:18.691615Z"}},"outputs":[{"name":"stdout","text":"tensor([[50256]])\n!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~����������������������������������������������������������������������������������������������\u0000\u0001\u0002\u0003\u0004\u0005\u0006\t\n\u000b\n impacted\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"print(tokenizer)\nprint(f\"model:{model}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:32:40.119687Z","iopub.execute_input":"2025-12-24T07:32:40.120265Z","iopub.status.idle":"2025-12-24T07:32:40.125268Z","shell.execute_reply.started":"2025-12-24T07:32:40.120231Z","shell.execute_reply":"2025-12-24T07:32:40.124571Z"}},"outputs":[{"name":"stdout","text":"GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)\nmodel:GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"# simplified text generation function\nprompt = \"Dear boss ...\"\n\ndef simple_text_generation(prompt,model,tokenizer,max_length=100):\n    input_ids =  tokenizer.encode(prompt,return_tensors=\"pt\") # pt = pytorch\n    # print(f\"prompt:{prompt}\\n input_ids: {input_ids}\")\n    outputs = model.generate(input_ids,max_length=100)\n    \n    # print(f\"generated result: {outputs}\")\n    \n    sentence = tokenizer.decode(outputs[0],skip_special_tokens=True)\n    # print(f\"outputs: {sentence}\")\n    return sentence\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:31:57.406883Z","iopub.execute_input":"2025-12-24T07:31:57.407198Z","iopub.status.idle":"2025-12-24T07:31:57.412035Z","shell.execute_reply.started":"2025-12-24T07:31:57.407167Z","shell.execute_reply":"2025-12-24T07:31:57.411391Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"prompt = \"Dear boss ...\"\ntext_generated = simple_text_generation(prompt,\n                                        model,\n                                        tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:31:57.413025Z","iopub.execute_input":"2025-12-24T07:31:57.413413Z","iopub.status.idle":"2025-12-24T07:32:00.943042Z","shell.execute_reply.started":"2025-12-24T07:31:57.413379Z","shell.execute_reply":"2025-12-24T07:32:00.942261Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(text_generated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:32:00.944796Z","iopub.execute_input":"2025-12-24T07:32:00.945125Z","iopub.status.idle":"2025-12-24T07:32:00.951225Z","shell.execute_reply.started":"2025-12-24T07:32:00.945085Z","shell.execute_reply":"2025-12-24T07:32:00.950482Z"}},"outputs":[{"name":"stdout","text":"Dear boss ... I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:32:12.905800Z","iopub.execute_input":"2025-12-24T07:32:12.906181Z","iopub.status.idle":"2025-12-24T07:32:12.910591Z","shell.execute_reply.started":"2025-12-24T07:32:12.906150Z","shell.execute_reply":"2025-12-24T07:32:12.909762Z"}},"outputs":[{"name":"stdout","text":"GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"data = sentences = [\n    \"this is all about tokenization\",\n    \"Tokenization transforms raw text into structured units called tokens, enabling language models to process sentences numerically while preserving linguistic meaning through consistent mapping between text fragments and integer identifiers.\",\n\n    \"Embedding layers convert token identifiers into dense continuous vectors, allowing neural networks to learn semantic similarity by placing related words closer together in high dimensional vector space during training.\",\n\n    \"Subword tokenization techniques such as byte pair encoding help models represent rare or unseen words by decomposing them into smaller meaningful units that still receive informative embeddings.\",\n\n    \"A tokenizer defines vocabulary size and token boundaries, directly influencing memory usage, sequence length, and the quality of embeddings learned by transformer based language models.\",\n\n    \"Word embeddings are learned parameters that capture semantic relationships, enabling models to infer meaning, analogy, and contextual relevance rather than treating words as isolated symbols.\",\n\n    \"When text is tokenized, punctuation, whitespace, and special characters are handled explicitly so the resulting token stream remains consistent across different inputs and training environments.\",\n\n    \"Embedding vectors are typically initialized randomly and gradually optimized through gradient descent so that contextual patterns in language are reflected in their numerical representations.\",\n\n    \"Tokenization choices affect downstream performance because poorly designed token splits can fragment meaning and make it harder for embedding layers to capture semantic coherence.\",\n\n    \"In transformer models, each token embedding represents a combination of lexical meaning and learned structure before positional information is added to encode word order.\",\n\n    \"Context independent embeddings assign one vector per token, while contextual embeddings adjust representations dynamically based on surrounding tokens within a sentence.\",\n\n    \"Padding tokens are introduced during tokenization to align sequence lengths in a batch, and their embeddings are usually masked to avoid influencing model predictions.\",\n\n    \"Embedding dimensions control the expressive power of a model, with larger dimensions allowing richer representations at the cost of increased computation and memory usage.\",\n\n    \"Tokenizers map text to integers deterministically, ensuring reproducibility so the same input sentence always produces identical token sequences across experiments.\",\n\n    \"Shared embedding spaces enable models to compare tokens mathematically, allowing cosine similarity or dot product operations to reveal semantic closeness between words.\",\n\n    \"Special tokens such as start of sequence and end of sequence guide models during training by clearly marking sentence boundaries in the tokenized input.\",\n\n    \"Tokenization errors propagate forward, meaning poorly segmented text can limit the quality of embeddings no matter how powerful the downstream neural architecture is.\",\n\n    \"Embedding layers act as a lookup table where each row corresponds to a token vector that is continuously refined as the model learns from large text datasets.\",\n\n    \"Character level tokenization avoids unknown words but increases sequence length dramatically, making embedding learning more computationally expensive for long inputs.\",\n\n    \"Subword embeddings strike a balance between vocabulary size and semantic granularity, making them effective for multilingual and low resource language modeling tasks.\",\n\n    \"During inference, tokenized text is passed through frozen embedding layers that transform symbolic input into numerical form suitable for matrix operations.\",\n\n    \"Embedding similarity allows models to generalize, so words appearing in similar contexts produce related vectors even if they never appear together explicitly.\",\n\n    \"Tokenization schemes differ across models, meaning embeddings trained with one tokenizer are generally incompatible with models expecting another vocabulary.\",\n\n    \"Learned embeddings encode both syntactic and semantic information, allowing models to understand grammatical roles as well as conceptual meaning.\",\n\n    \"Byte level tokenization ensures every possible input can be represented, but often produces longer token sequences requiring careful embedding optimization.\",\n\n    \"Embedding matrices can be inspected directly in frameworks like PyTorch, revealing how tokens correspond to rows of trainable numerical parameters.\",\n\n    \"Tokenization converts unstructured text into a discrete representation that neural networks can efficiently batch, embed, and process in parallel.\",\n\n    \"Pretrained embeddings provide a strong initialization that helps models converge faster by starting from linguistically informed representations.\",\n\n    \"Token embeddings are shared across all occurrences of a token, allowing consistent meaning to be reinforced across many training examples.\",\n\n    \"Positional embeddings are added to token embeddings so models can distinguish between identical tokens appearing at different positions in a sequence.\",\n\n    \"The quality of embeddings depends heavily on data diversity, since richer corpora expose tokens to varied contexts that shape their vector representations.\",\n\n    \"Tokenization must balance linguistic accuracy with computational efficiency to avoid unnecessary fragmentation of common words.\",\n\n    \"Embedding vectors live in continuous space, enabling smooth interpolation between meanings rather than rigid categorical distinctions.\",\n\n    \"Tokenizers handle casing rules differently, meaning lowercasing text can significantly impact embedding reuse and vocabulary size.\",\n\n    \"In causal language models, token embeddings are optimized to predict the next token, reinforcing contextual relationships through training objectives.\",\n\n    \"Embedding lookup is one of the first operations in a language model forward pass, transforming integer inputs into floating point tensors.\",\n\n    \"Subword tokenization helps reduce out of vocabulary issues while allowing embeddings to capture meaningful morphological patterns.\",\n\n    \"Embedding layers are typically followed by attention mechanisms that refine token representations based on interactions with neighboring tokens.\",\n\n    \"Tokenization defines how text is segmented, but embeddings determine how those segments are understood numerically by the model.\",\n\n    \"Training embeddings jointly with the model allows them to adapt to task specific language usage rather than remaining static.\",\n\n    \"Token embeddings encode prior knowledge learned during pretraining, enabling downstream tasks to benefit from general language understanding.\",\n\n    \"Whitespace handling during tokenization affects how embeddings represent word boundaries and sentence structure.\",\n\n    \"Embedding normalization techniques can improve stability by keeping vector magnitudes within reasonable bounds.\",\n\n    \"Tokenizers must be deterministic so embedding lookup remains consistent across distributed training environments.\",\n\n    \"Embedding similarity can reveal biases present in training data, as tokens reflecting similar contexts cluster together.\",\n\n    \"Special tokens receive their own embeddings, allowing models to treat structural markers differently from regular text tokens.\",\n\n    \"Tokenization errors often appear subtle but can degrade embedding quality in long sequences.\",\n\n    \"Embedding matrices grow linearly with vocabulary size, making efficient tokenization essential for scaling large models.\",\n\n    \"Contextual embeddings evolve across transformer layers, refining token meaning as more context is incorporated.\",\n\n    \"Tokenization is language dependent, so multilingual models rely heavily on shared subword embeddings.\",\n\n    \"Embedding layers are differentiable components that learn through gradient updates during backpropagation.\",\n\n    \"Token frequency influences embedding quality, as rare tokens receive fewer updates during training.\",\n\n    \"Embedding inspection helps researchers understand how models internalize linguistic structure.\",\n\n    \"Tokenizers define how numbers, symbols, and punctuation are represented before embedding lookup.\",\n\n    \"Embedding vectors allow models to compute relationships using linear algebra rather than symbolic rules.\",\n\n    \"Tokenization choices influence sequence length, which directly impacts attention complexity.\",\n\n    \"Embedding layers compress discrete token identities into dense numerical forms suitable for neural computation.\",\n\n    \"Poor tokenization can increase sequence length unnecessarily, reducing embedding efficiency.\",\n\n    \"Embedding sharing between input and output layers reduces parameters and improves generalization.\",\n\n    \"Tokenization determines the granularity at which meaning is represented in embeddings.\",\n\n    \"Embedding spaces often capture analogical relationships such as semantic similarity or oppositeness.\",\n\n    \"Tokenization pipelines must remain consistent between training and inference to preserve embedding alignment.\",\n\n    \"Embedding learning benefits from large corpora where tokens appear in diverse linguistic contexts.\",\n\n    \"Tokenizers may split words differently depending on prefixes, suffixes, or frequency statistics.\",\n\n    \"Embedding vectors are updated incrementally as models learn from prediction errors.\",\n\n    \"Tokenization is a preprocessing step, but embeddings are learned representations within the model.\",\n\n    \"Embedding dimensionality reflects a tradeoff between expressiveness and computational cost.\",\n\n    \"Tokenization errors are difficult to correct after embedding lookup has occurred.\",\n\n    \"Embedding layers serve as the bridge between symbolic language and numerical computation.\",\n\n    \"Tokenizers encode language rules implicitly through their vocabulary construction process.\",\n\n    \"Embedding similarity metrics enable semantic search and clustering applications.\",\n\n    \"Tokenization defines model input structure, while embeddings define representational meaning.\",\n\n    \"Embedding matrices can be visualized to analyze semantic clustering of tokens.\",\n\n    \"Tokenization must handle edge cases like emojis, URLs, and code snippets consistently.\",\n\n    \"Embedding vectors evolve during training to reflect task specific linguistic patterns.\",\n\n    \"Tokenization consistency ensures embeddings remain meaningful across different datasets.\",\n\n    \"Embedding layers translate discrete token indices into continuous feature representations.\",\n\n    \"Tokenization granularity affects how efficiently embeddings encode meaning.\",\n\n    \"Embedding quality is tightly coupled with tokenizer design decisions.\",\n\n    \"Tokenization strategies influence how models generalize to unseen text.\",\n\n    \"Embedding learning enables neural models to capture language structure without explicit rules.\",\n\n    \"Tokenization converts language into a format embeddings can transform into meaning.\",\n\n    \"Embedding layers are foundational components underlying modern natural language processing systems.\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:11:20.198058Z","iopub.execute_input":"2025-12-24T08:11:20.198432Z","iopub.status.idle":"2025-12-24T08:11:20.208681Z","shell.execute_reply.started":"2025-12-24T08:11:20.198402Z","shell.execute_reply":"2025-12-24T08:11:20.207847Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# all inputs must have the same length\n# add a dummy token at the end\n# Having the same length => this is called padding\n\ntokenizer.pad_token = tokenizer.eos_token \n\ntokenized_data = [tokenizer.encode_plus(\n    sentence,\n    add_special_tokens= True,\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    max_length=50,\n    \n) for sentence in data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:11:26.569364Z","iopub.execute_input":"2025-12-24T08:11:26.569662Z","iopub.status.idle":"2025-12-24T08:11:26.594324Z","shell.execute_reply.started":"2025-12-24T08:11:26.569636Z","shell.execute_reply":"2025-12-24T08:11:26.593734Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"tokenized_data[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:12:51.485132Z","iopub.execute_input":"2025-12-24T08:12:51.485433Z","iopub.status.idle":"2025-12-24T08:12:51.492644Z","shell.execute_reply.started":"2025-12-24T08:12:51.485409Z","shell.execute_reply":"2025-12-24T08:12:51.492006Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"[{'input_ids': tensor([[ 5661,   318,   477,   546, 11241,  1634, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]])},\n {'input_ids': tensor([[30642,  1634, 31408,  8246,  2420,   656, 20793,  4991,  1444, 16326,\n             11, 15882,  3303,  4981,   284,  1429, 13439,  5470,  1146,   981,\n          23934, 29929,  3616,   832,  6414, 16855,  1022,  2420, 21441,   290,\n          18253, 42814,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]])}]"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}