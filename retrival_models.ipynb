{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"authorship_tag":"ABX9TyMbKGTK525va4gtoTXtzK31","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Probablistic retrival model, Fundamental of RAG","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:50.661542Z","iopub.execute_input":"2025-12-25T19:33:50.662247Z","iopub.status.idle":"2025-12-25T19:33:50.665950Z","shell.execute_reply.started":"2025-12-25T19:33:50.662215Z","shell.execute_reply":"2025-12-25T19:33:50.665248Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Initialize the tokenizer and the model\nmodel_id = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:50.688432Z","iopub.execute_input":"2025-12-25T19:33:50.688895Z","iopub.status.idle":"2025-12-25T19:33:51.471479Z","shell.execute_reply.started":"2025-12-25T19:33:50.688871Z","shell.execute_reply":"2025-12-25T19:33:51.470915Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"model.transformer.wte.weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:51.472687Z","iopub.execute_input":"2025-12-25T19:33:51.472910Z","iopub.status.idle":"2025-12-25T19:33:51.479430Z","shell.execute_reply.started":"2025-12-25T19:33:51.472887Z","shell.execute_reply":"2025-12-25T19:33:51.478781Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n        ...,\n        [-0.0445, -0.0548,  0.0123,  ...,  0.1044,  0.0978, -0.0695],\n        [ 0.1860,  0.0167,  0.0461,  ..., -0.0963,  0.0785, -0.0225],\n        [ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207]],\n       requires_grad=True)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"print(tokenizer.encode(\"<|endoftext|>\",return_tensors=\"pt\"))\nprint(tokenizer.decode(range(200)))\nprint(tokenizer.decode([20755]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:51.480433Z","iopub.execute_input":"2025-12-25T19:33:51.480790Z","iopub.status.idle":"2025-12-25T19:33:51.495246Z","shell.execute_reply.started":"2025-12-25T19:33:51.480758Z","shell.execute_reply":"2025-12-25T19:33:51.494686Z"}},"outputs":[{"name":"stdout","text":"tensor([[50256]])\n!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~����������������������������������������������������������������������������������������������\u0000\u0001\u0002\u0003\u0004\u0005\u0006\t\n\u000b\n impacted\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(tokenizer)\nprint(f\"model:{model}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:51.496461Z","iopub.execute_input":"2025-12-25T19:33:51.496702Z","iopub.status.idle":"2025-12-25T19:33:51.509715Z","shell.execute_reply.started":"2025-12-25T19:33:51.496681Z","shell.execute_reply":"2025-12-25T19:33:51.509065Z"}},"outputs":[{"name":"stdout","text":"GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)\nmodel:GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"# simplified text generation function\nprompt = \"Dear boss ...\"\n\ndef simple_text_generation(prompt,model,tokenizer,max_length=100):\n    input_ids =  tokenizer.encode(prompt,return_tensors=\"pt\") # pt = pytorch\n    # print(f\"prompt:{prompt}\\n input_ids: {input_ids}\")\n    outputs = model.generate(input_ids,max_length=100)\n    \n    # print(f\"generated result: {outputs}\")\n    \n    sentence = tokenizer.decode(outputs[0],skip_special_tokens=True)\n    # print(f\"outputs: {sentence}\")\n    return sentence\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:51.510544Z","iopub.execute_input":"2025-12-25T19:33:51.510852Z","iopub.status.idle":"2025-12-25T19:33:51.526758Z","shell.execute_reply.started":"2025-12-25T19:33:51.510831Z","shell.execute_reply":"2025-12-25T19:33:51.526068Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"prompt = \"Dear boss ...\"\ntext_generated = simple_text_generation(prompt,\n                                        model,\n                                        tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:51.528232Z","iopub.execute_input":"2025-12-25T19:33:51.528483Z","iopub.status.idle":"2025-12-25T19:33:54.453266Z","shell.execute_reply.started":"2025-12-25T19:33:51.528462Z","shell.execute_reply":"2025-12-25T19:33:54.452654Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"print(text_generated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.454522Z","iopub.execute_input":"2025-12-25T19:33:54.454799Z","iopub.status.idle":"2025-12-25T19:33:54.459206Z","shell.execute_reply.started":"2025-12-25T19:33:54.454776Z","shell.execute_reply":"2025-12-25T19:33:54.458454Z"}},"outputs":[{"name":"stdout","text":"Dear boss ... I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"print(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.460235Z","iopub.execute_input":"2025-12-25T19:33:54.460450Z","iopub.status.idle":"2025-12-25T19:33:54.487805Z","shell.execute_reply.started":"2025-12-25T19:33:54.460429Z","shell.execute_reply":"2025-12-25T19:33:54.487099Z"}},"outputs":[{"name":"stdout","text":"GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"data = sentences = [\n    \"this is all about tokenization\",\n    \"Tokenization transforms raw text into structured units called tokens, enabling language models to process sentences numerically while preserving linguistic meaning through consistent mapping between text fragments and integer identifiers.\",\n\n    \"Embedding layers convert token identifiers into dense continuous vectors, allowing neural networks to learn semantic similarity by placing related words closer together in high dimensional vector space during training.\",\n\n    \"Subword tokenization techniques such as byte pair encoding help models represent rare or unseen words by decomposing them into smaller meaningful units that still receive informative embeddings.\",\n\n    \"A tokenizer defines vocabulary size and token boundaries, directly influencing memory usage, sequence length, and the quality of embeddings learned by transformer based language models.\",\n\n    \"Word embeddings are learned parameters that capture semantic relationships, enabling models to infer meaning, analogy, and contextual relevance rather than treating words as isolated symbols.\",\n\n    \"When text is tokenized, punctuation, whitespace, and special characters are handled explicitly so the resulting token stream remains consistent across different inputs and training environments.\",\n\n    \"Embedding vectors are typically initialized randomly and gradually optimized through gradient descent so that contextual patterns in language are reflected in their numerical representations.\",\n\n    \"Tokenization choices affect downstream performance because poorly designed token splits can fragment meaning and make it harder for embedding layers to capture semantic coherence.\",\n\n    \"In transformer models, each token embedding represents a combination of lexical meaning and learned structure before positional information is added to encode word order.\",\n\n    \"Context independent embeddings assign one vector per token, while contextual embeddings adjust representations dynamically based on surrounding tokens within a sentence.\",\n\n    \"Padding tokens are introduced during tokenization to align sequence lengths in a batch, and their embeddings are usually masked to avoid influencing model predictions.\",\n\n    \"Embedding dimensions control the expressive power of a model, with larger dimensions allowing richer representations at the cost of increased computation and memory usage.\",\n\n    \"Tokenizers map text to integers deterministically, ensuring reproducibility so the same input sentence always produces identical token sequences across experiments.\",\n\n    \"Shared embedding spaces enable models to compare tokens mathematically, allowing cosine similarity or dot product operations to reveal semantic closeness between words.\",\n\n    \"Special tokens such as start of sequence and end of sequence guide models during training by clearly marking sentence boundaries in the tokenized input.\",\n\n    \"Tokenization errors propagate forward, meaning poorly segmented text can limit the quality of embeddings no matter how powerful the downstream neural architecture is.\",\n\n    \"Embedding layers act as a lookup table where each row corresponds to a token vector that is continuously refined as the model learns from large text datasets.\",\n\n    \"Character level tokenization avoids unknown words but increases sequence length dramatically, making embedding learning more computationally expensive for long inputs.\",\n\n    \"Subword embeddings strike a balance between vocabulary size and semantic granularity, making them effective for multilingual and low resource language modeling tasks.\",\n\n    \"During inference, tokenized text is passed through frozen embedding layers that transform symbolic input into numerical form suitable for matrix operations.\",\n\n    \"Embedding similarity allows models to generalize, so words appearing in similar contexts produce related vectors even if they never appear together explicitly.\",\n\n    \"Tokenization schemes differ across models, meaning embeddings trained with one tokenizer are generally incompatible with models expecting another vocabulary.\",\n\n    \"Learned embeddings encode both syntactic and semantic information, allowing models to understand grammatical roles as well as conceptual meaning.\",\n\n    \"Byte level tokenization ensures every possible input can be represented, but often produces longer token sequences requiring careful embedding optimization.\",\n\n    \"Embedding matrices can be inspected directly in frameworks like PyTorch, revealing how tokens correspond to rows of trainable numerical parameters.\",\n\n    \"Tokenization converts unstructured text into a discrete representation that neural networks can efficiently batch, embed, and process in parallel.\",\n\n    \"Pretrained embeddings provide a strong initialization that helps models converge faster by starting from linguistically informed representations.\",\n\n    \"Token embeddings are shared across all occurrences of a token, allowing consistent meaning to be reinforced across many training examples.\",\n\n    \"Positional embeddings are added to token embeddings so models can distinguish between identical tokens appearing at different positions in a sequence.\",\n\n    \"The quality of embeddings depends heavily on data diversity, since richer corpora expose tokens to varied contexts that shape their vector representations.\",\n\n    \"Tokenization must balance linguistic accuracy with computational efficiency to avoid unnecessary fragmentation of common words.\",\n\n    \"Embedding vectors live in continuous space, enabling smooth interpolation between meanings rather than rigid categorical distinctions.\",\n\n    \"Tokenizers handle casing rules differently, meaning lowercasing text can significantly impact embedding reuse and vocabulary size.\",\n\n    \"In causal language models, token embeddings are optimized to predict the next token, reinforcing contextual relationships through training objectives.\",\n\n    \"Embedding lookup is one of the first operations in a language model forward pass, transforming integer inputs into floating point tensors.\",\n\n    \"Subword tokenization helps reduce out of vocabulary issues while allowing embeddings to capture meaningful morphological patterns.\",\n\n    \"Embedding layers are typically followed by attention mechanisms that refine token representations based on interactions with neighboring tokens.\",\n\n    \"Tokenization defines how text is segmented, but embeddings determine how those segments are understood numerically by the model.\",\n\n    \"Training embeddings jointly with the model allows them to adapt to task specific language usage rather than remaining static.\",\n\n    \"Token embeddings encode prior knowledge learned during pretraining, enabling downstream tasks to benefit from general language understanding.\",\n\n    \"Whitespace handling during tokenization affects how embeddings represent word boundaries and sentence structure.\",\n\n    \"Embedding normalization techniques can improve stability by keeping vector magnitudes within reasonable bounds.\",\n\n    \"Tokenizers must be deterministic so embedding lookup remains consistent across distributed training environments.\",\n\n    \"Embedding similarity can reveal biases present in training data, as tokens reflecting similar contexts cluster together.\",\n\n    \"Special tokens receive their own embeddings, allowing models to treat structural markers differently from regular text tokens.\",\n\n    \"Tokenization errors often appear subtle but can degrade embedding quality in long sequences.\",\n\n    \"Embedding matrices grow linearly with vocabulary size, making efficient tokenization essential for scaling large models.\",\n\n    \"Contextual embeddings evolve across transformer layers, refining token meaning as more context is incorporated.\",\n\n    \"Tokenization is language dependent, so multilingual models rely heavily on shared subword embeddings.\",\n\n    \"Embedding layers are differentiable components that learn through gradient updates during backpropagation.\",\n\n    \"Token frequency influences embedding quality, as rare tokens receive fewer updates during training.\",\n\n    \"Embedding inspection helps researchers understand how models internalize linguistic structure.\",\n\n    \"Tokenizers define how numbers, symbols, and punctuation are represented before embedding lookup.\",\n\n    \"Embedding vectors allow models to compute relationships using linear algebra rather than symbolic rules.\",\n\n    \"Tokenization choices influence sequence length, which directly impacts attention complexity.\",\n\n    \"Embedding layers compress discrete token identities into dense numerical forms suitable for neural computation.\",\n\n    \"Poor tokenization can increase sequence length unnecessarily, reducing embedding efficiency.\",\n\n    \"Embedding sharing between input and output layers reduces parameters and improves generalization.\",\n\n    \"Tokenization determines the granularity at which meaning is represented in embeddings.\",\n\n    \"Embedding spaces often capture analogical relationships such as semantic similarity or oppositeness.\",\n\n    \"Tokenization pipelines must remain consistent between training and inference to preserve embedding alignment.\",\n\n    \"Embedding learning benefits from large corpora where tokens appear in diverse linguistic contexts.\",\n\n    \"Tokenizers may split words differently depending on prefixes, suffixes, or frequency statistics.\",\n\n    \"Embedding vectors are updated incrementally as models learn from prediction errors.\",\n\n    \"Tokenization is a preprocessing step, but embeddings are learned representations within the model.\",\n\n    \"Embedding dimensionality reflects a tradeoff between expressiveness and computational cost.\",\n\n    \"Tokenization errors are difficult to correct after embedding lookup has occurred.\",\n\n    \"Embedding layers serve as the bridge between symbolic language and numerical computation.\",\n\n    \"Tokenizers encode language rules implicitly through their vocabulary construction process.\",\n\n    \"Embedding similarity metrics enable semantic search and clustering applications.\",\n\n    \"Tokenization defines model input structure, while embeddings define representational meaning.\",\n\n    \"Embedding matrices can be visualized to analyze semantic clustering of tokens.\",\n\n    \"Tokenization must handle edge cases like emojis, URLs, and code snippets consistently.\",\n\n    \"Embedding vectors evolve during training to reflect task specific linguistic patterns.\",\n\n    \"Tokenization consistency ensures embeddings remain meaningful across different datasets.\",\n\n    \"Embedding layers translate discrete token indices into continuous feature representations.\",\n\n    \"Tokenization granularity affects how efficiently embeddings encode meaning.\",\n\n    \"Embedding quality is tightly coupled with tokenizer design decisions.\",\n\n    \"Tokenization strategies influence how models generalize to unseen text.\",\n\n    \"Embedding learning enables neural models to capture language structure without explicit rules.\",\n\n    \"Tokenization converts language into a format embeddings can transform into meaning.\",\n\n    \"Embedding layers are foundational components underlying modern natural language processing systems.\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.488756Z","iopub.execute_input":"2025-12-25T19:33:54.489126Z","iopub.status.idle":"2025-12-25T19:33:54.507555Z","shell.execute_reply.started":"2025-12-25T19:33:54.489094Z","shell.execute_reply":"2025-12-25T19:33:54.506898Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# all inputs must have the same length\n# add a dummy token at the end\n# Having the same length => this is called padding\n\ntokenizer.pad_token = tokenizer.eos_token \n\ntokenized_data = [tokenizer.encode_plus(\n    sentence,\n    add_special_tokens= True,\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    max_length=50,\n    \n) for sentence in data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.510326Z","iopub.execute_input":"2025-12-25T19:33:54.510812Z","iopub.status.idle":"2025-12-25T19:33:54.556551Z","shell.execute_reply.started":"2025-12-25T19:33:54.510789Z","shell.execute_reply":"2025-12-25T19:33:54.556086Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"tokenized_data[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.557261Z","iopub.execute_input":"2025-12-25T19:33:54.557526Z","iopub.status.idle":"2025-12-25T19:33:54.563605Z","shell.execute_reply.started":"2025-12-25T19:33:54.557496Z","shell.execute_reply":"2025-12-25T19:33:54.563091Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"[{'input_ids': tensor([[ 5661,   318,   477,   546, 11241,  1634, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]])},\n {'input_ids': tensor([[30642,  1634, 31408,  8246,  2420,   656, 20793,  4991,  1444, 16326,\n             11, 15882,  3303,  4981,   284,  1429, 13439,  5470,  1146,   981,\n          23934, 29929,  3616,   832,  6414, 16855,  1022,  2420, 21441,   290,\n          18253, 42814,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]])}]"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"input_ids = [item[\"input_ids\"].squeeze() for item in tokenized_data]\nattention_masks = [mask[\"attention_mask\"].squeeze() for mask in tokenized_data]\ninput_ids[:5],attention_masks[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.564487Z","iopub.execute_input":"2025-12-25T19:33:54.565218Z","iopub.status.idle":"2025-12-25T19:33:54.583851Z","shell.execute_reply.started":"2025-12-25T19:33:54.565196Z","shell.execute_reply":"2025-12-25T19:33:54.583171Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"([tensor([ 5661,   318,   477,   546, 11241,  1634, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  tensor([30642,  1634, 31408,  8246,  2420,   656, 20793,  4991,  1444, 16326,\n             11, 15882,  3303,  4981,   284,  1429, 13439,  5470,  1146,   981,\n          23934, 29929,  3616,   832,  6414, 16855,  1022,  2420, 21441,   290,\n          18253, 42814,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  tensor([31567,  6048,   278, 11685, 10385, 11241, 42814,   656, 15715, 12948,\n          30104,    11,  5086, 17019,  7686,   284,  2193, 37865, 26789,   416,\n          12560,  3519,  2456,  5699,  1978,   287,  1029, 38517, 15879,  2272,\n           1141,  3047,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  tensor([ 7004,  4775, 11241,  1634,  7605,   884,   355, 18022,  5166, 21004,\n           1037,  4981,  2380,  4071,   393, 29587,  2456,   416, 26969, 32927,\n            606,   656,  4833, 11570,  4991,   326,   991,  3328, 30304, 11525,\n             67,   654,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  tensor([   32, 11241,  7509, 15738, 25818,  2546,   290, 11241, 13215,    11,\n           3264, 32596,  4088,  8748,    11,  8379,  4129,    11,   290,   262,\n           3081,   286, 11525,    67,   654,  4499,   416, 47385,  1912,  3303,\n           4981,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])],\n [tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0])])"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"### Convert the input_ids and attention mask to tensors","metadata":{}},{"cell_type":"code","source":"inputs_ids_tensor = torch.stack(input_ids)\nattention_masks_tensor = torch.stack(attention_masks)\ninputs_ids_tensor[:3],attention_masks_tensor[:3]\n\nprint(inputs_ids_tensor.shape)\n# print(input_ids.shape) # python list doesn't have shape property, that is why we change to torch tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.584754Z","iopub.execute_input":"2025-12-25T19:33:54.585204Z","iopub.status.idle":"2025-12-25T19:33:54.599533Z","shell.execute_reply.started":"2025-12-25T19:33:54.585181Z","shell.execute_reply":"2025-12-25T19:33:54.599003Z"}},"outputs":[{"name":"stdout","text":"torch.Size([83, 50])\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"padded_input_ids = pad_sequence(inputs_ids_tensor,\n             batch_first=True,\n             padding_value=tokenizer.eos_token_id)\npadded_attention_masks = pad_sequence(attention_masks_tensor,\n                                     batch_first=True,\n                                     padding_value=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.600305Z","iopub.execute_input":"2025-12-25T19:33:54.600582Z","iopub.status.idle":"2025-12-25T19:33:54.616470Z","shell.execute_reply.started":"2025-12-25T19:33:54.600553Z","shell.execute_reply":"2025-12-25T19:33:54.615971Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"padded_input_ids[:2],padded_attention_masks[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.617946Z","iopub.execute_input":"2025-12-25T19:33:54.618165Z","iopub.status.idle":"2025-12-25T19:33:54.640905Z","shell.execute_reply.started":"2025-12-25T19:33:54.618145Z","shell.execute_reply":"2025-12-25T19:33:54.640355Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 5661,   318,   477,   546, 11241,  1634, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n         [30642,  1634, 31408,  8246,  2420,   656, 20793,  4991,  1444, 16326,\n             11, 15882,  3303,  4981,   284,  1429, 13439,  5470,  1146,   981,\n          23934, 29929,  3616,   832,  6414, 16855,  1022,  2420, 21441,   290,\n          18253, 42814,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]),\n tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]]))"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self,input_ids,attention_masks):\n        self.input_ids = input_ids\n        self.attention_masks = attention_masks\n        self.labels = input_ids.clone()\n    def __len__(self):\n        return len(self.input_ids)\n    def __getitem__(self,index):\n        return {\n            \"input_ids\":self.input_ids[index],\n            \"attention_mask\":self.attention_masks[index],\n            \"labels\":self.labels[index]\n        }\n\n\ndataset = TextDataset(inputs_ids_tensor,attention_masks_tensor)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.641585Z","iopub.execute_input":"2025-12-25T19:33:54.641869Z","iopub.status.idle":"2025-12-25T19:33:54.655322Z","shell.execute_reply.started":"2025-12-25T19:33:54.641836Z","shell.execute_reply":"2025-12-25T19:33:54.654847Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"len(dataset),dataset[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.656089Z","iopub.execute_input":"2025-12-25T19:33:54.656320Z","iopub.status.idle":"2025-12-25T19:33:54.673994Z","shell.execute_reply.started":"2025-12-25T19:33:54.656298Z","shell.execute_reply":"2025-12-25T19:33:54.673306Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"(83,\n {'input_ids': tensor([31567,  6048,   278, 11685, 10385, 11241, 42814,   656, 15715, 12948,\n          30104,    11,  5086, 17019,  7686,   284,  2193, 37865, 26789,   416,\n          12560,  3519,  2456,  5699,  1978,   287,  1029, 38517, 15879,  2272,\n           1141,  3047,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  'labels': tensor([31567,  6048,   278, 11685, 10385, 11241, 42814,   656, 15715, 12948,\n          30104,    11,  5086, 17019,  7686,   284,  2193, 37865, 26789,   416,\n          12560,  3519,  2456,  5699,  1978,   287,  1029, 38517, 15879,  2272,\n           1141,  3047,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])})"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"### Fine tuning the GPT2 model\n","metadata":{}},{"cell_type":"code","source":"data_loader = DataLoader(dataset,batch_size=2,shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.674807Z","iopub.execute_input":"2025-12-25T19:33:54.675154Z","iopub.status.idle":"2025-12-25T19:33:54.688054Z","shell.execute_reply.started":"2025-12-25T19:33:54.675122Z","shell.execute_reply":"2025-12-25T19:33:54.687533Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# data_loader\n# for batch in data_loader:\n#     print(batch)\n#     print(\"\\n\"*5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.688786Z","iopub.execute_input":"2025-12-25T19:33:54.689049Z","iopub.status.idle":"2025-12-25T19:33:54.705847Z","shell.execute_reply.started":"2025-12-25T19:33:54.689021Z","shell.execute_reply":"2025-12-25T19:33:54.705305Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"model.parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.706636Z","iopub.execute_input":"2025-12-25T19:33:54.706847Z","iopub.status.idle":"2025-12-25T19:33:54.723037Z","shell.execute_reply.started":"2025-12-25T19:33:54.706827Z","shell.execute_reply":"2025-12-25T19:33:54.722443Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"<bound method Module.parameters of GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(),lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.723924Z","iopub.execute_input":"2025-12-25T19:33:54.724176Z","iopub.status.idle":"2025-12-25T19:33:54.756632Z","shell.execute_reply.started":"2025-12-25T19:33:54.724155Z","shell.execute_reply":"2025-12-25T19:33:54.756161Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Set the model to training mode\nmodel.train()\n\n# Training loop\nfor epoch in range(10):\n    for batch in data_loader:\n        # Unpacking the input and atttention mask ids\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        # Reset the gradients to zero\n        optimizer.zero_grad()\n        #forward pass\n        outputs = model(input_ids=input_ids,\n                       attention_mask=attention_mask,\n                       labels=input_ids)\n        loss = outputs.loss\n        #backward pass\n        loss.backward()\n        #update the model parameters\n        optimizer.step()\n    # print the loss for the current epoch to monitor the progress\n    print(f\"Epoch {epoch+1} -Loss: {loss.item()}\")\n    \n        \n\n        \n\n\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:33:54.757287Z","iopub.execute_input":"2025-12-25T19:33:54.757521Z","iopub.status.idle":"2025-12-25T19:43:31.667411Z","shell.execute_reply.started":"2025-12-25T19:33:54.757493Z","shell.execute_reply":"2025-12-25T19:43:31.666693Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 -Loss: 1.399208903312683\nEpoch 2 -Loss: 1.386404275894165\nEpoch 3 -Loss: 1.8037998676300049\nEpoch 4 -Loss: 1.418811559677124\nEpoch 5 -Loss: 0.7961050271987915\nEpoch 6 -Loss: 0.4337821900844574\nEpoch 7 -Loss: 0.6178490519523621\nEpoch 8 -Loss: 0.1530536413192749\nEpoch 9 -Loss: 0.21940605342388153\nEpoch 10 -Loss: 0.21999230980873108\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"### Define funciton to generate text","metadata":{}},{"cell_type":"code","source":"def generate_text(prompt,model,tokenizer,max_length=100):\n    inputs = tokenizer.encode_plus(prompt,return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n    outputs = model.generate(input_ids,\n                             attention_mask=attention_mask,\n                             max_length=max_length)\n    return tokenizer.decode(outputs[0],skip_special_tokens=True)\n\nprompt = \"what is Embedding?\"\n\ntext_generated = generate_text(prompt,model,tokenizer,max_length=500)\nprint(f\"text_generated: {text_generated}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:43:31.668406Z","iopub.execute_input":"2025-12-25T19:43:31.668931Z","iopub.status.idle":"2025-12-25T19:43:31.740207Z","shell.execute_reply.started":"2025-12-25T19:43:31.668902Z","shell.execute_reply":"2025-12-25T19:43:31.739536Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"text_generated: what is Embedding?\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"## Tokenization and Embeddings","metadata":{}},{"cell_type":"code","source":"# install the faiss-cpu library\n!pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:43:31.741121Z","iopub.execute_input":"2025-12-25T19:43:31.741410Z","iopub.status.idle":"2025-12-25T19:43:39.840878Z","shell.execute_reply.started":"2025-12-25T19:43:31.741376Z","shell.execute_reply":"2025-12-25T19:43:39.840030Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\nDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.13.2\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"import faiss\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer,AutoModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:43:39.842224Z","iopub.execute_input":"2025-12-25T19:43:39.842549Z","iopub.status.idle":"2025-12-25T19:43:39.892144Z","shell.execute_reply.started":"2025-12-25T19:43:39.842513Z","shell.execute_reply":"2025-12-25T19:43:39.891456Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Initialize the tokenizer and model for generating embeddings\nmodel_id = \"sentence-transformers/paraphrase-MiniLM-L6-V2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ngen_model = AutoModel.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:43:39.893051Z","iopub.execute_input":"2025-12-25T19:43:39.893318Z","iopub.status.idle":"2025-12-25T19:43:43.078461Z","shell.execute_reply.started":"2025-12-25T19:43:39.893297Z","shell.execute_reply":"2025-12-25T19:43:43.077917Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2d34bf9685c4c3b8c5d4c34bcce118c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05735767a5194ad8b1e69d22f626b830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e78dbdc60c40489f61df8980eb42e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed06feead02c48ae90a4e32c3704147e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea7185db6ca2493cbe0473d4fb3e98ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31afb30f1dc84bbc81a36de58c17194d"}},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"documents = [\n    \"Ethiopia’s telecom liberalization after 2021 introduced Safaricom Ethiopia alongside Ethio Telecom, reshaping mobile connectivity, digital payments, tower sharing, and SIM registration practices in secondary cities and border towns that previously relied on unreliable single-operator coverage.\",\n\n    \"The Grand Ethiopian Renaissance Dam progressed through multiple coordinated filling and power-generation stages between 2020 and 2024, influencing internal grid stability, seasonal energy planning, and cross-border electricity exports rather than remaining only a geopolitical dispute symbol.\",\n\n    \"Addis Ababa’s Sheger River project combined flood mitigation, riverbank rehabilitation, pedestrian corridors, and informal settlement relocation, changing real estate demand and urban mobility patterns in western neighborhoods long excluded from earlier master plan priorities.\",\n\n    \"Ethiopia’s specialty coffee reform allowed cooperatives in Sidama, Guji, and Yirgacheffe to export directly using digital traceability systems, preserving micro-lot identities and farmer premiums that were previously diluted through centralized auction blending under the ECX framework.\",\n\n    \"The Homegrown Economic Reform II agenda focused on forex market adjustments, partial subsidy removals, and state-owned enterprise restructuring, creating short-term inflation pressures while aiming to attract foreign investment beyond traditional infrastructure-heavy public spending models.\",\n\n    \"The expansion of Addis Ababa’s bus rapid transit corridors altered daily commuting patterns for low-income residents, reducing dependence on informal minibuses and reshaping peak-hour congestion dynamics along routes connecting peripheral condominium housing developments.\",\n\n    \"Ethiopia’s national digital ID initiative, Fayda, began phased rollouts to support financial inclusion, SIM registration, and public service access, introducing biometric verification challenges in rural kebeles with limited connectivity and electricity reliability.\",\n\n    \"The conflict-driven disruptions in northern Ethiopia significantly affected agricultural cycles, seed distribution, and local markets, producing uneven recovery patterns across zones depending on road access, humanitarian logistics, and regional administrative coordination.\",\n\n    \"Ethiopia’s leather and footwear sector faced export volatility due to changing global demand, domestic power interruptions, and regulatory shifts that affected foreign-owned factories operating in industrial parks such as Hawassa and Bole Lemi.\",\n\n    \"Urban condominium housing programs in Addis Ababa reshaped household asset ownership and rental markets, but also introduced challenges around maintenance funding, vertical community governance, and distance from employment centers for lower-income beneficiaries.\",\n\n    \"Ethiopia’s renewable energy strategy increasingly emphasized wind and geothermal projects alongside hydropower, particularly in Rift Valley sites, to diversify generation sources and reduce vulnerability to seasonal rainfall variability.\",\n\n    \"The Addis–Djibouti electric railway struggled with operational sustainability, facing power supply inconsistencies, foreign exchange shortages for spare parts, and underutilized freight capacity despite its strategic importance for Ethiopia’s import–export corridor.\",\n\n    \"Ethiopia’s digital startup ecosystem expanded modestly after 2020, driven by fintech, logistics, and agri-tech ventures, yet constrained by limited venture capital availability, regulatory uncertainty, and difficulties integrating with legacy state-dominated systems.\",\n\n    \"The reform of Ethiopia’s fuel pricing system introduced gradual cost pass-through mechanisms, affecting transportation fares, food prices, and household budgets, while attempting to reduce fiscal burdens previously absorbed through government subsidies.\",\n\n    \"Regional state boundary disputes in Ethiopia increasingly intersected with administrative decentralization policies, complicating service delivery, voter registration, and local security coordination during periods of political transition.\",\n\n    \"The growth of mobile money platforms such as Telebirr accelerated digital transactions among informal traders, public transport operators, and rural merchants, gradually reducing cash dependency despite intermittent network outages.\",\n\n    \"Ethiopia’s higher education expansion produced a growing number of computer science and engineering graduates, yet many faced skill mismatches due to limited industry collaboration and uneven access to modern computing infrastructure.\",\n\n    \"The commercialization of agriculture corridors emphasized wheat self-sufficiency initiatives, promoting mechanization and irrigation in lowland areas while creating tensions over land tenure, pastoralist mobility, and water access.\",\n\n    \"Ethiopia’s urban food distribution relied heavily on informal markets, where price volatility reflected transport costs, fuel prices, and seasonal supply disruptions rather than centralized price-setting mechanisms.\",\n\n    \"The introduction of electronic tax registers aimed to broaden Ethiopia’s tax base, but implementation challenges emerged among small enterprises unfamiliar with digital accounting systems and facing inconsistent electricity supply.\",\n\n    \"Addis Ababa’s ride-hailing market evolved through local platforms and global competitors, navigating regulatory ambiguity, driver classification debates, and fare affordability constraints for middle- and low-income users.\",\n\n    \"Ethiopia’s aviation sector remained regionally dominant through Ethiopian Airlines, whose cargo operations expanded significantly during global supply chain disruptions, offsetting declines in passenger travel demand.\",\n\n    \"The restructuring of state-owned banks prioritized balance sheet cleanup and digital service expansion, yet credit access for small businesses remained constrained by collateral requirements and risk-averse lending practices.\",\n\n    \"Climate variability increasingly affected Ethiopia’s pastoralist communities, forcing adaptive migration patterns and straining traditional conflict resolution mechanisms over grazing land and water resources.\",\n\n    \"The proliferation of satellite television and online media altered political discourse in Ethiopia, creating fragmented information environments that challenged traditional state-controlled broadcasting narratives.\",\n\n    \"Ethiopia’s industrial park strategy aimed to attract export-oriented manufacturing, but retention of foreign firms depended on consistent utilities, customs efficiency, and predictable labor relations frameworks.\",\n\n    \"The rapid spread of smartphones among Ethiopian youth accelerated social media usage, influencing language trends, music distribution, and informal online commerce despite periodic internet disruptions.\",\n\n    \"Ethiopia’s road infrastructure expansion improved interregional connectivity, yet maintenance backlogs and axle-load enforcement issues limited long-term transport efficiency gains.\",\n\n    \"Public health system reforms emphasized primary care expansion, but uneven staffing and supply chain constraints continued to affect service quality in remote woredas.\",\n\n    \"Ethiopia’s traditional coffee ceremonies persisted as social institutions, even as urban lifestyles shortened preparation times and adapted rituals to apartment living spaces.\",\n\n    \"The push for wheat irrigation in dry regions highlighted tensions between national food security goals and local environmental sustainability considerations.\",\n\n    \"Ethiopia’s music industry increasingly leveraged digital streaming platforms, reducing dependence on physical media while navigating monetization challenges due to limited local payment integration.\",\n\n    \"Cross-border trade with neighboring countries relied heavily on informal networks, shaped by currency shortages, security conditions, and fluctuating customs enforcement practices.\",\n\n    \"Ethiopia’s urban waste management initiatives introduced pilot recycling programs, yet struggled with public awareness, informal waste pickers’ integration, and landfill capacity constraints.\",\n\n    \"The adoption of electric cooking initiatives aimed to reduce biomass dependency, but uptake was slowed by appliance costs and unreliable household power connections.\",\n\n    \"Ethiopia’s microfinance institutions expanded digital loan disbursement, balancing financial inclusion goals against rising default risks during economic shocks.\",\n\n    \"The growth of private universities altered higher education access, but raised concerns about instructional quality and graduate employability alignment.\",\n\n    \"Ethiopia’s textile value chain remained vulnerable to global cotton price fluctuations and shipping delays affecting export timelines.\",\n\n    \"Urban youth unemployment contributed to the rise of informal gig work, including delivery services and freelance digital tasks.\",\n\n    \"Ethiopia’s public procurement digitization aimed to reduce corruption risks, but adoption varied across ministries and regional bureaus.\",\n\n    \"The expansion of solar mini-grids supported rural electrification, though long-term maintenance financing remained uncertain.\",\n\n    \"Ethiopia’s linguistic diversity continued to shape regional media consumption and education policy implementation.\",\n\n    \"Addis Ababa’s real estate boom created affordability gaps between formal housing and informal settlements.\",\n\n    \"The normalization of remote work remained limited by bandwidth costs and power reliability.\",\n\n    \"Ethiopia’s freight logistics faced container shortages during global trade disruptions.\",\n\n    \"Agricultural extension services increasingly used mobile messaging for farmer outreach.\",\n\n    \"The integration of refugees into local economies varied by region and policy support.\",\n\n    \"Urban flooding events highlighted drainage infrastructure weaknesses in fast-growing neighborhoods.\",\n\n    \"Ethiopia’s artisanal mining sector contributed to local incomes while posing environmental risks.\",\n\n    \"Digital learning platforms expanded during school closures, exposing access inequalities.\",\n\n    \"The expansion of expressway toll roads altered long-distance transport economics.\",\n\n    \"Ethiopia’s seed certification reforms aimed to improve crop yields.\",\n\n    \"Public–private partnerships gained attention in infrastructure financing debates.\",\n\n    \"The use of drones for agricultural monitoring emerged in pilot programs.\",\n\n    \"Ethiopia’s creative economy policy discussions emphasized cultural exports.\",\n\n    \"Urban water supply projects struggled with non-revenue water losses.\",\n\n    \"The rise of local podcasting reflected youth-led media experimentation.\",\n\n    \"Ethiopia’s climate adaptation planning integrated community-based early warning systems.\",\n\n    \"The retail sector saw gradual adoption of point-of-sale technologies.\",\n\n    \"Cross-regional labor migration influenced urban housing demand.\",\n\n    \"Ethiopia’s judicial digitization pilots aimed to reduce case backlogs.\",\n\n    \"Public transport fare adjustments reflected inflationary pressures.\",\n\n    \"The expansion of technical vocational training targeted manufacturing skills gaps.\",\n\n    \"Ethiopia’s horticulture exports relied on cold-chain logistics improvements.\",\n\n    \"Urban air quality concerns grew alongside vehicle imports.\",\n\n    \"Ethiopia’s e-commerce sector faced last-mile delivery challenges.\",\n\n    \"The integration of traditional dispute resolution into formal systems persisted.\",\n\n    \"Renewable energy financing increasingly involved blended finance models.\",\n\n    \"Ethiopia’s census delays affected planning accuracy.\",\n\n    \"The expansion of call centers created new urban employment niches.\",\n\n    \"Digital mapping projects improved disaster response coordination.\",\n\n    \"Ethiopia’s film industry experimented with international co-productions.\",\n\n    \"Urban green spaces influenced neighborhood social interactions.\",\n\n    \"The rise of online education influencers shaped student aspirations.\",\n\n    \"Ethiopia’s rail freight pricing affected industrial competitiveness.\",\n\n    \"Public sector performance contracts aimed to improve accountability.\",\n\n    \"The growth of coworking spaces reflected changing work culture.\",\n\n    \"Ethiopia’s border trade posts integrated digital customs systems.\",\n\n    \"Agricultural insurance pilots addressed climate risk exposure.\",\n\n    \"Urban noise pollution became a policy discussion topic.\",\n\n    \"Ethiopia’s diaspora remittances shifted toward digital channels.\",\n\n    \"The expansion of data centers supported emerging cloud services.\",\n\n    \"Public libraries experimented with digital resource access.\",\n\n    \"Ethiopia’s urban cycling initiatives remained limited but symbolic.\",\n\n    \"The coordination of humanitarian logistics relied on shared data platforms.\",\n\n    \"Ethiopia’s innovation hubs fostered early-stage tech experimentation.\",\n\n    \"Urban planning debates increasingly included climate resilience metrics.\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:44:33.070675Z","iopub.execute_input":"2025-12-25T19:44:33.070977Z","iopub.status.idle":"2025-12-25T19:44:33.081090Z","shell.execute_reply.started":"2025-12-25T19:44:33.070952Z","shell.execute_reply":"2025-12-25T19:44:33.080484Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def embed_text(text,tokenizer,model):\n    inputs = tokenizer(text,\n                       return_tensors=\"pt\",\n                       padding=True,\n                       truncation=True\n                       \n                      )\n    with torch.no_grad():\n        embeddings = model(**inputs).last_hidden_state # last context riched tokens\n        embeddings = embeddings.mean(dim=1) # pooling token embeddings into single sentence embedding for the seek of retrieval system\n    return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:49:43.670020Z","iopub.execute_input":"2025-12-25T19:49:43.670349Z","iopub.status.idle":"2025-12-25T19:49:43.675375Z","shell.execute_reply.started":"2025-12-25T19:49:43.670319Z","shell.execute_reply":"2025-12-25T19:49:43.674575Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"gen_model,tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:55:13.379923Z","iopub.execute_input":"2025-12-25T19:55:13.380673Z","iopub.status.idle":"2025-12-25T19:55:13.386904Z","shell.execute_reply.started":"2025-12-25T19:55:13.380629Z","shell.execute_reply":"2025-12-25T19:55:13.386171Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"(BertModel(\n   (embeddings): BertEmbeddings(\n     (word_embeddings): Embedding(30522, 384, padding_idx=0)\n     (position_embeddings): Embedding(512, 384)\n     (token_type_embeddings): Embedding(2, 384)\n     (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n     (dropout): Dropout(p=0.1, inplace=False)\n   )\n   (encoder): BertEncoder(\n     (layer): ModuleList(\n       (0-5): 6 x BertLayer(\n         (attention): BertAttention(\n           (self): BertSdpaSelfAttention(\n             (query): Linear(in_features=384, out_features=384, bias=True)\n             (key): Linear(in_features=384, out_features=384, bias=True)\n             (value): Linear(in_features=384, out_features=384, bias=True)\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n           (output): BertSelfOutput(\n             (dense): Linear(in_features=384, out_features=384, bias=True)\n             (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (intermediate): BertIntermediate(\n           (dense): Linear(in_features=384, out_features=1536, bias=True)\n           (intermediate_act_fn): GELUActivation()\n         )\n         (output): BertOutput(\n           (dense): Linear(in_features=1536, out_features=384, bias=True)\n           (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n       )\n     )\n   )\n   (pooler): BertPooler(\n     (dense): Linear(in_features=384, out_features=384, bias=True)\n     (activation): Tanh()\n   )\n ),\n BertTokenizerFast(name_or_path='sentence-transformers/paraphrase-MiniLM-L6-V2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n }\n ))"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"# Initialize a list to store the bembeddings\ndocument_embeddings = []\n\nfor doc in documents:\n    doc_embeddings = embed_text(doc,tokenizer,gen_model)\n    document_embeddings.append(doc_embeddings)\n\n\ndocument_embeddings[1].shape\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:03:45.906874Z","iopub.execute_input":"2025-12-25T20:03:45.907169Z","iopub.status.idle":"2025-12-25T20:03:47.431934Z","shell.execute_reply.started":"2025-12-25T20:03:45.907142Z","shell.execute_reply":"2025-12-25T20:03:47.431287Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 384])"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":" document_embeddings = torch.cat(document_embeddings).cpu().numpy()\ndocument_embeddings.shape, document_embeddings[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:03:50.426792Z","iopub.execute_input":"2025-12-25T20:03:50.427094Z","iopub.status.idle":"2025-12-25T20:03:50.440696Z","shell.execute_reply.started":"2025-12-25T20:03:50.427065Z","shell.execute_reply":"2025-12-25T20:03:50.440012Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"((87, 384),\n array([[-1.60419613e-01, -4.24365066e-02, -1.68803677e-01,\n         -2.31677130e-01,  1.48904890e-01, -9.44776237e-02,\n         -2.03818262e-01, -6.98364573e-03, -9.51517522e-02,\n          3.84154767e-01,  2.14787990e-01,  1.89004317e-01,\n         -1.86711699e-01, -2.05783024e-02,  1.38443440e-01,\n          2.30668366e-01, -2.14105502e-01, -3.23954791e-01,\n         -1.45259991e-01, -5.45860529e-01, -1.52238719e-02,\n         -4.24987942e-01, -2.30765656e-01,  1.05395913e-02,\n          7.43249655e-02, -1.53370261e-01, -8.42300802e-03,\n         -1.65376291e-01,  3.62123609e-01,  1.69244617e-01,\n          2.39755839e-01,  4.90767688e-01,  4.46761921e-02,\n          8.12464133e-02,  2.94345301e-02, -1.63322419e-01,\n         -1.32990807e-01,  1.00306027e-01, -1.07720688e-01,\n         -3.49458829e-02,  4.15672570e-01, -2.36408375e-02,\n         -1.25645742e-01, -2.89088607e-01,  2.71396697e-01,\n          1.29683524e-01,  1.49547711e-01,  3.80161345e-01,\n         -2.33667120e-01, -3.98546338e-01,  4.24578518e-01,\n          3.19906175e-01, -2.03163877e-01, -1.30685508e-01,\n         -4.94487464e-01, -1.25146687e-01, -3.27118069e-01,\n          1.05907708e-01,  3.96078795e-01,  5.31138182e-02,\n         -2.22323358e-01, -1.91210229e-02, -2.30873860e-02,\n          1.16342321e-01, -3.24432582e-01, -4.96090390e-03,\n          1.13860615e-01, -6.85425848e-02, -5.23759685e-02,\n          1.39786407e-01,  5.94112976e-03, -5.17234564e-01,\n         -8.74187797e-02,  4.38109666e-01, -9.33312923e-02,\n         -9.83556062e-02,  4.68352944e-01,  2.76679814e-01,\n          2.07742780e-01, -5.50603271e-02, -1.05920918e-01,\n          4.14396584e-01,  2.65194327e-01, -1.77903265e-01,\n          2.39599317e-01, -1.37683183e-01,  7.03463480e-02,\n          7.39910603e-02,  1.10907525e-01, -4.38590556e-01,\n          4.78809208e-01,  2.10957423e-01,  2.07937613e-01,\n          1.66465595e-01,  9.12575796e-02,  2.55411178e-01,\n          2.13877201e-01, -1.74256817e-01, -1.86119407e-01,\n          4.28943429e-03, -2.13827759e-01,  4.92168702e-02,\n          2.50322610e-01,  8.33177641e-02, -3.72472703e-02,\n         -2.54404023e-02,  8.04589167e-02, -3.72705571e-02,\n          5.11342704e-01,  9.93769169e-02, -3.05304557e-01,\n         -1.76914353e-02, -3.06482539e-02, -1.78242832e-01,\n          5.24812192e-02, -1.61113650e-01, -2.35167801e-01,\n          1.31142605e-02,  3.74302119e-01,  2.11628586e-01,\n         -2.92037278e-01, -5.42172909e-01, -2.18019262e-03,\n         -9.76867676e-02, -1.54612198e-01,  4.37861383e-01,\n          1.77839413e-01, -1.50930975e-02, -6.79073036e-02,\n         -1.42150864e-01, -2.27224052e-01,  2.06221953e-01,\n         -1.19715564e-01,  1.28485709e-01,  1.37131274e-01,\n         -2.94385940e-01,  3.20820920e-02,  5.25077768e-02,\n         -3.72149646e-01, -2.27351993e-01, -1.98980421e-01,\n          2.73461640e-01,  4.16693658e-01, -2.00114220e-01,\n          2.14253098e-01,  2.95653902e-02,  1.76155061e-01,\n          1.69087648e-01, -1.66091621e-02, -6.05760098e-01,\n          2.30550364e-01, -1.13736562e-01,  3.54460061e-01,\n          4.41403806e-01,  3.22642505e-01, -1.52792305e-01,\n          1.98798552e-01,  2.98255473e-01,  5.26884980e-02,\n         -3.70659769e-01, -1.04230996e-02, -3.34881186e-01,\n         -3.87005985e-01, -5.93903437e-02,  9.55486968e-02,\n         -1.38415903e-01, -3.87718141e-01,  1.78568792e-02,\n         -2.68928081e-01,  7.43570849e-02, -4.71804291e-01,\n         -3.96777332e-01, -5.84398471e-02, -1.40421450e-01,\n          1.00277014e-01, -7.22795948e-02, -6.89463243e-02,\n          1.43378630e-01, -5.34344673e-01, -3.72819453e-02,\n         -1.20332085e-01, -5.21598935e-01,  6.12734966e-02,\n         -2.03142330e-01,  2.35477882e-03,  3.11515364e-03,\n          3.97225708e-01, -3.03132504e-01,  3.39822620e-01,\n         -2.54519790e-01, -2.60957956e-01, -7.21664280e-02,\n          6.54413223e-01, -1.17071860e-01,  1.68730557e-01,\n          5.58810867e-02,  2.30873954e-02, -4.68500018e-01,\n          2.01304778e-01, -2.88160175e-01, -2.96646029e-01,\n          2.65713990e-01, -3.21949542e-01, -2.46093497e-02,\n          1.13382369e-01,  3.25755715e-01,  3.79603133e-02,\n         -5.47388531e-02, -2.56518841e-01,  4.59669560e-01,\n          5.32598078e-01,  8.84355828e-02,  5.16523004e-01,\n          3.81903201e-02,  1.94547027e-01,  1.15995690e-01,\n         -1.03298314e-01,  4.84340563e-02, -1.46033689e-01,\n          1.34890050e-01, -1.76183358e-02,  4.78635281e-01,\n          2.87241906e-01,  2.35478953e-02, -2.99524069e-01,\n         -3.88062559e-02,  6.36595339e-02, -3.65519583e-01,\n         -2.35524878e-01, -8.61461684e-02, -1.02031007e-01,\n          3.32346149e-02,  2.16048330e-01, -1.17874138e-01,\n         -5.37665524e-02,  2.69429777e-02,  4.28922206e-01,\n         -1.94248274e-01, -1.03196494e-01, -5.05099483e-02,\n         -1.30713001e-01,  2.03739643e-01, -2.76226342e-01,\n         -7.79523998e-02, -1.26599342e-01, -1.90038279e-01,\n         -1.02945969e-01, -3.56416800e-03, -2.44137734e-01,\n          1.49809765e-02, -3.58775914e-01,  1.52643040e-01,\n          3.95098269e-01,  1.43110566e-02,  9.94245186e-02,\n         -6.88885280e-04, -1.45686250e-02,  1.32108152e-01,\n          9.12090987e-02, -4.44730446e-02,  2.66413808e-01,\n         -1.10363945e-01,  7.06633669e-04, -2.73586027e-02,\n          6.06530190e-01,  6.94731995e-02, -1.30929872e-01,\n         -3.49122211e-02,  3.44882369e-01, -7.76553154e-03,\n         -3.15593094e-01,  3.58511388e-01, -2.80265480e-01,\n         -3.60824645e-01,  1.88094601e-01,  2.98996747e-01,\n          6.09836020e-02,  2.09229887e-01,  1.26056418e-01,\n         -2.23341689e-01,  1.93602189e-01,  2.83065170e-01,\n         -3.59450936e-01, -2.16817558e-01, -1.12255134e-01,\n         -3.11941326e-01,  9.84607786e-02,  2.61076510e-01,\n          1.22927064e-02,  1.93793043e-01,  2.07358837e-01,\n         -3.31438407e-02,  4.43603843e-01,  7.99136534e-02,\n          1.98336363e-01,  1.01961859e-01, -4.64961946e-01,\n         -6.34176373e-01, -2.03165218e-01,  2.20247909e-01,\n          6.00185134e-02,  4.46415782e-01,  1.09427840e-01,\n          1.73813850e-01, -2.14602232e-01, -5.28178290e-02,\n          1.96195811e-01, -4.35329974e-01, -1.07362363e-02,\n         -1.18306912e-01,  4.08476800e-01, -2.68705577e-01,\n         -2.84428179e-01,  1.93392299e-02, -2.41807222e-01,\n         -1.13291040e-01, -5.85251689e-01,  7.20419586e-01,\n          3.76317471e-01,  1.54818157e-02,  1.60063595e-01,\n         -3.54287378e-03,  1.57094464e-01,  6.96030185e-02,\n          9.98663902e-03,  2.45273456e-01, -6.95072114e-02,\n         -4.45994377e-01, -4.51162495e-02,  5.34961596e-02,\n         -2.52923042e-01,  1.37812391e-01, -1.67439863e-01,\n          2.74702668e-01,  2.14164227e-01,  2.06347615e-01,\n          4.31363508e-02,  1.47673234e-01,  3.01278755e-02,\n          2.17393339e-01,  2.57789820e-01,  1.07135311e-01,\n          2.18079671e-01,  3.88062924e-01,  9.38762128e-02,\n         -2.16845021e-01,  5.22873819e-01,  2.73084700e-01,\n          4.14312154e-01, -2.08935007e-01, -1.63121670e-01,\n         -2.82406006e-02, -5.36030903e-02,  1.18003242e-01,\n         -2.83187360e-01, -1.13977194e-01, -4.83860314e-01,\n         -2.00363085e-01, -1.52474970e-01, -3.59011143e-01,\n          4.75629538e-01, -1.20933823e-01, -1.38458550e-01,\n         -8.41138437e-02,  3.94271404e-01, -3.06773305e-01,\n         -1.19338267e-01,  6.43970072e-02, -3.43887776e-01,\n         -4.02427047e-01, -3.58140618e-01,  3.30700316e-02,\n          5.21022342e-02, -1.72396600e-01,  2.78013665e-02,\n         -6.63525283e-01, -2.46121541e-01,  2.38752052e-01,\n         -1.58055991e-01, -1.74632203e-02, -2.92650729e-01,\n         -9.38297883e-02, -7.62953237e-02,  3.16340812e-02],\n        [-1.89540163e-01,  1.44859865e-01, -2.63681337e-02,\n         -9.01886895e-02,  5.02782874e-02, -4.00729150e-01,\n         -2.57219911e-01, -3.93938534e-02, -2.02147111e-01,\n          2.51092494e-01, -2.41311729e-01,  6.39629588e-02,\n         -1.63777500e-01, -6.58715963e-02, -8.13759789e-02,\n          3.82774323e-01, -4.52681720e-01, -1.43980548e-01,\n         -3.00075084e-01, -2.16414854e-01,  2.21095741e-01,\n         -2.62700051e-01, -2.21276417e-01,  3.24549042e-02,\n          9.04030502e-02,  2.82781512e-01, -7.45168924e-02,\n          2.45262027e-01,  2.00932756e-01, -1.20378055e-01,\n         -3.30558009e-02,  1.17283747e-01, -5.29865086e-01,\n          1.21821649e-01, -2.27614380e-02,  3.32284689e-01,\n         -4.39980961e-02, -4.30934690e-02,  1.97245598e-01,\n          3.32025513e-02,  2.63650209e-01, -3.81514072e-01,\n         -7.94485491e-03, -1.24288201e-01,  1.64809853e-01,\n          4.52950299e-01,  3.41096520e-01, -1.31859645e-01,\n         -3.69746417e-01, -1.95205569e-01,  3.28741223e-01,\n         -1.84885830e-01,  1.65193439e-01, -4.47048813e-01,\n          2.64989445e-03, -7.13490769e-02, -3.72677110e-02,\n         -4.42349672e-01,  3.44525784e-01, -6.66718706e-02,\n         -2.80539602e-01, -2.30079070e-01,  2.53438026e-01,\n          1.01112969e-01, -1.54938936e-01, -2.13863939e-01,\n         -2.29490697e-02,  3.63089263e-01,  2.29132269e-02,\n          6.64865449e-02,  1.07379287e-01, -2.32776716e-01,\n          8.65562726e-03, -4.47610050e-01, -1.04965232e-01,\n         -1.98835373e-01,  1.39877409e-01,  2.26817369e-01,\n         -1.88827634e-01, -2.40935266e-01,  1.05951846e-01,\n          1.92468047e-01, -1.99008361e-01, -2.96575844e-01,\n          1.56841427e-01,  1.38534427e-01,  3.48937571e-01,\n         -3.27174701e-02,  2.72852600e-01, -2.02683762e-01,\n          2.67814219e-01,  2.55813450e-01,  8.93198699e-02,\n          2.86220521e-01,  1.27010360e-01,  3.10198367e-01,\n          1.79712445e-01, -4.02501643e-01, -3.22320983e-02,\n         -1.87967181e-01, -2.08932981e-02,  3.13102067e-01,\n          6.89929128e-02, -2.83535540e-01,  1.91198528e-01,\n          1.36619881e-01, -1.38474509e-01,  3.48424494e-01,\n          3.63631286e-02, -3.65198553e-01,  2.35487163e-01,\n         -4.25638080e-01, -4.18516994e-02,  9.55251679e-02,\n         -2.71401405e-01, -2.72559464e-01, -4.26930845e-01,\n         -8.41004401e-02, -7.34810755e-02, -7.00289831e-02,\n         -2.76411623e-02, -3.57809186e-01, -2.19057262e-01,\n          5.49159236e-02, -1.06867038e-01,  2.18495250e-01,\n          1.57941356e-01,  4.30708751e-02, -2.17818588e-01,\n         -4.74143744e-01, -2.92008929e-02,  3.34298774e-03,\n         -2.41189077e-01,  1.17863566e-01, -5.91617599e-02,\n          9.90175679e-02, -3.26892026e-02,  4.63084280e-02,\n         -1.13754747e-02, -5.51423207e-02, -2.67109841e-01,\n          2.99684703e-01, -1.70246243e-01, -4.73613948e-01,\n         -1.48138637e-02, -1.99388564e-01,  4.94686440e-02,\n          8.98063034e-02,  2.75560245e-02, -2.60289073e-01,\n          2.61485968e-02, -3.19886118e-01,  8.30701590e-02,\n          3.30468506e-01,  4.73327756e-01, -1.95962191e-01,\n         -1.35423586e-01,  2.67051667e-01,  3.93208951e-01,\n         -1.72468573e-01,  2.38022562e-02,  9.42178667e-02,\n         -6.00259483e-01, -6.72999993e-02, -1.99640870e-01,\n          9.83442296e-04, -2.15180084e-01,  9.99292126e-04,\n          5.19460961e-02, -7.50747472e-02, -7.76808783e-02,\n         -4.99444269e-02,  4.11012443e-03,  1.36269644e-01,\n          4.16304171e-01,  1.60908923e-01, -2.60672003e-01,\n          9.40546989e-02,  1.02684781e-01,  4.27851737e-01,\n          2.52044559e-01,  1.96205433e-02,  3.98439527e-01,\n          2.87404452e-02, -1.16451792e-01,  5.76442247e-03,\n          2.75990009e-01, -4.41391133e-02, -2.77406782e-01,\n         -8.50430503e-02, -4.48715687e-01,  6.41074916e-03,\n          3.67044210e-01,  3.93992484e-01,  2.15093106e-01,\n          1.94788039e-01,  2.70244360e-01, -1.57268092e-01,\n          1.24414511e-01, -2.40951687e-01, -3.49324167e-01,\n          1.58235535e-01, -4.08760428e-01,  1.34590775e-01,\n         -3.57594937e-02,  4.54093888e-02,  1.13788061e-01,\n          2.94533938e-01, -4.62574542e-01,  2.77101278e-01,\n          1.76422641e-01, -1.38962805e-01,  3.15664321e-01,\n          1.64300669e-02,  5.38947582e-01, -8.35914314e-02,\n         -1.53576359e-02, -2.76654154e-01, -3.64847273e-01,\n          9.97883156e-02,  4.97401386e-01,  1.82301581e-01,\n          4.15891171e-01, -2.02703699e-02,  1.37673736e-01,\n         -3.87531482e-02, -2.08213210e-01,  5.17409481e-02,\n          2.02190340e-01,  3.79064590e-01,  6.76969588e-02,\n         -1.47754088e-01, -1.93522781e-01,  3.24043542e-01,\n         -1.29791573e-01,  2.30909452e-01,  2.53702879e-01,\n         -8.35485682e-02, -2.55937278e-01, -1.25376150e-01,\n         -3.38232458e-01, -7.27722868e-02, -1.47805244e-01,\n          7.98365027e-02,  4.24036980e-02,  2.87418455e-01,\n         -2.55962253e-01,  1.63272023e-01, -1.72714055e-01,\n         -1.61322474e-01, -1.28083229e-01, -3.18395734e-01,\n         -4.74865735e-02,  1.28241405e-01, -2.99921185e-01,\n          1.55839235e-01, -1.90393135e-01,  2.86325626e-02,\n         -1.39861286e-01,  4.44867089e-02,  3.21202166e-02,\n         -9.89782065e-02,  2.19553202e-01,  1.48555398e-01,\n          2.69462794e-01,  1.62575409e-01,  3.41214269e-01,\n          2.51675069e-01, -3.29345673e-01,  2.74917096e-01,\n         -3.03423822e-01,  2.15866327e-01, -1.19903468e-01,\n          6.99051768e-02, -1.25390351e-01,  9.64326784e-02,\n          1.87903598e-01, -1.57826796e-01,  3.99639159e-01,\n         -4.61836755e-01,  2.83676147e-01,  2.22578183e-01,\n         -6.61794245e-01,  1.86813459e-01, -1.06061913e-01,\n         -2.26324961e-01,  2.36909658e-01,  9.94503275e-02,\n          3.53549211e-03,  7.53984153e-02,  4.51722234e-01,\n         -2.01731890e-01,  2.41242543e-01, -1.34657435e-02,\n          1.75451010e-01,  5.20164706e-02, -3.62812787e-01,\n         -3.35754305e-01,  1.38567626e-01,  4.40360233e-02,\n         -2.13079005e-02,  3.07608813e-01,  3.58200371e-01,\n          4.77701128e-02, -2.16994226e-01,  1.54108033e-01,\n          4.86798078e-01,  1.63611583e-02, -6.37693629e-02,\n         -3.44700336e-01, -8.26245323e-02, -2.15019166e-01,\n         -1.05452292e-01,  1.30478293e-01,  1.45903245e-01,\n         -7.54376948e-02, -3.54629725e-01,  1.50173038e-01,\n          8.76365677e-02,  2.98924185e-02, -4.78193723e-02,\n          2.99003899e-01,  1.23574235e-01,  1.31856427e-01,\n         -3.41479838e-01,  1.49506912e-01,  1.07285790e-01,\n          1.23262621e-01, -1.64248347e-01,  2.51341099e-03,\n         -2.96187084e-02,  1.74481771e-03,  6.82860836e-02,\n          1.68840975e-01, -1.51306108e-01, -1.53707772e-01,\n          6.27678782e-02,  1.46873146e-01, -1.97701469e-01,\n         -6.98505342e-02, -9.53705162e-02, -5.61789498e-02,\n          2.82037944e-01,  1.72801346e-01, -2.35355362e-01,\n         -8.18205923e-02,  2.48907149e-01,  3.05830032e-01,\n         -6.78462014e-02, -4.25165027e-01,  1.46192521e-01,\n         -3.65370288e-02, -2.32356146e-01,  3.13176326e-02,\n          1.92679957e-01,  1.44534171e-01, -2.79570699e-01,\n          1.50952905e-01, -1.28505304e-01, -2.92169154e-01,\n          1.94428638e-01,  2.59255111e-01,  4.44114953e-02,\n         -3.08751911e-02,  1.09063787e-03,  3.34105156e-02,\n         -3.83926749e-01,  1.96926355e-01, -1.63920805e-01,\n         -5.73825166e-02,  7.57781938e-02, -4.05931734e-02,\n          1.95014179e-01, -1.44243360e-01,  2.06853926e-01,\n         -1.86009601e-01, -4.13167953e-01,  3.75335217e-01,\n         -1.52093053e-01, -1.49494886e-01, -2.65921932e-02,\n          1.28616959e-01,  9.61052552e-02,  6.93148598e-02]], dtype=float32))"},"metadata":{}}],"execution_count":74},{"cell_type":"markdown","source":"### Build the Retrieval System","metadata":{}},{"cell_type":"code","source":"document_embeddings.shape[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:20:38.767114Z","iopub.execute_input":"2025-12-25T20:20:38.767399Z","iopub.status.idle":"2025-12-25T20:20:38.772747Z","shell.execute_reply.started":"2025-12-25T20:20:38.767371Z","shell.execute_reply":"2025-12-25T20:20:38.771681Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"384"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":" index = faiss.IndexFlatL2(document_embeddings.shape[1])\nindex.add(document_embeddings)\nprint(index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:21:05.056063Z","iopub.execute_input":"2025-12-25T20:21:05.056872Z","iopub.status.idle":"2025-12-25T20:21:05.060649Z","shell.execute_reply.started":"2025-12-25T20:21:05.056840Z","shell.execute_reply":"2025-12-25T20:21:05.059836Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"faiss.downcast_index(index)\nindex.d, index.ntotal, index.is_trained,index.metric_type","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:26:47.279795Z","iopub.execute_input":"2025-12-25T20:26:47.280088Z","iopub.status.idle":"2025-12-25T20:26:47.285139Z","shell.execute_reply.started":"2025-12-25T20:26:47.280061Z","shell.execute_reply":"2025-12-25T20:26:47.284437Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"(384, 87, True, 1)"},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"# Retrieval -> Build a function to retrieve information\ndef retrieve(query,tokenizer,model,index,documents,top_k=3):\n    query_embeddings = embed_text(query,tokenizer,model)\n    distances, indices = index.search(query_embeddings,top_k)\n    return [documents[i] for i in indices[0]],distances[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:41:07.095340Z","iopub.execute_input":"2025-12-25T20:41:07.095671Z","iopub.status.idle":"2025-12-25T20:41:07.100032Z","shell.execute_reply.started":"2025-12-25T20:41:07.095608Z","shell.execute_reply":"2025-12-25T20:41:07.099207Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# Test the retrieval function\nquery = \"what is news about ethiotelecom\"\nretrieved_docs,distances = retrieve(query,tokenizer,gen_model,index,documents)\nfor d in retrieved_docs:\n    print(d)\n    print(\"\\n\"*5)\nprint(distances)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:46:19.037753Z","iopub.execute_input":"2025-12-25T20:46:19.038420Z","iopub.status.idle":"2025-12-25T20:46:19.061969Z","shell.execute_reply.started":"2025-12-25T20:46:19.038390Z","shell.execute_reply":"2025-12-25T20:46:19.061207Z"}},"outputs":[{"name":"stdout","text":"Ethiopia’s telecom liberalization after 2021 introduced Safaricom Ethiopia alongside Ethio Telecom, reshaping mobile connectivity, digital payments, tower sharing, and SIM registration practices in secondary cities and border towns that previously relied on unreliable single-operator coverage.\n\n\n\n\n\n\nAddis Ababa’s ride-hailing market evolved through local platforms and global competitors, navigating regulatory ambiguity, driver classification debates, and fare affordability constraints for middle- and low-income users.\n\n\n\n\n\n\nThe rapid spread of smartphones among Ethiopian youth accelerated social media usage, influencing language trends, music distribution, and informal online commerce despite periodic internet disruptions.\n\n\n\n\n\n\n[50.294266 52.196934 53.72694 ]\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}