{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"authorship_tag":"ABX9TyMbKGTK525va4gtoTXtzK31","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Probablistic retrival model, Fundamental of RAG","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:34:54.210088Z","iopub.execute_input":"2025-12-26T17:34:54.210305Z","iopub.status.idle":"2025-12-26T17:35:07.620125Z","shell.execute_reply.started":"2025-12-26T17:34:54.210279Z","shell.execute_reply":"2025-12-26T17:35:07.619303Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Initialize the tokenizer and the model\nmodel_id = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:07.621843Z","iopub.execute_input":"2025-12-26T17:35:07.622307Z","iopub.status.idle":"2025-12-26T17:35:29.007016Z","shell.execute_reply.started":"2025-12-26T17:35:07.622277Z","shell.execute_reply":"2025-12-26T17:35:29.006196Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4727a3890e48d1b22a24a45fc72ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4409cb69120a4836871d5e6b98a4b768"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1633e65465fb44e4be52cc0f6458fce3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ec312ab5ed4482a9f9f488441061d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78c098bc3f634eb7a3dcd42b52401e54"}},"metadata":{}},{"name":"stderr","text":"2025-12-26 17:35:11.908732: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766770512.121822      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766770512.186083      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766770512.716893      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766770512.716932      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766770512.716935      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766770512.716938      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1b980e3b6544728408e64424161ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"182e78d9de374ee58d2cb48e9ba0924d"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model.transformer.wte.weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:29.008176Z","iopub.execute_input":"2025-12-26T17:35:29.009095Z","iopub.status.idle":"2025-12-26T17:35:29.037098Z","shell.execute_reply.started":"2025-12-26T17:35:29.009066Z","shell.execute_reply":"2025-12-26T17:35:29.036336Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n        ...,\n        [-0.0445, -0.0548,  0.0123,  ...,  0.1044,  0.0978, -0.0695],\n        [ 0.1860,  0.0167,  0.0461,  ..., -0.0963,  0.0785, -0.0225],\n        [ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207]],\n       requires_grad=True)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"print(tokenizer.encode(\"<|endoftext|>\",return_tensors=\"pt\"))\nprint(tokenizer.decode(range(200)))\nprint(tokenizer.decode([20755]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:29.038161Z","iopub.execute_input":"2025-12-26T17:35:29.038566Z","iopub.status.idle":"2025-12-26T17:35:29.049940Z","shell.execute_reply.started":"2025-12-26T17:35:29.038536Z","shell.execute_reply":"2025-12-26T17:35:29.049290Z"}},"outputs":[{"name":"stdout","text":"tensor([[50256]])\n!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~����������������������������������������������������������������������������������������������\u0000\u0001\u0002\u0003\u0004\u0005\u0006\t\n\u000b\n impacted\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(tokenizer)\nprint(f\"model:{model}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:29.050983Z","iopub.execute_input":"2025-12-26T17:35:29.051397Z","iopub.status.idle":"2025-12-26T17:35:29.057179Z","shell.execute_reply.started":"2025-12-26T17:35:29.051359Z","shell.execute_reply":"2025-12-26T17:35:29.056416Z"}},"outputs":[{"name":"stdout","text":"GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)\nmodel:GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"# simplified text generation function\nprompt = \"Dear boss ...\"\n\ndef simple_text_generation(prompt,model,tokenizer,max_length=100):\n    input_ids =  tokenizer.encode(prompt,return_tensors=\"pt\") # pt = pytorch\n    # print(f\"prompt:{prompt}\\n input_ids: {input_ids}\")\n    outputs = model.generate(input_ids,max_length=100)\n    \n    # print(f\"generated result: {outputs}\")\n    \n    sentence = tokenizer.decode(outputs[0],skip_special_tokens=True)\n    # print(f\"outputs: {sentence}\")\n    return sentence\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:29.058086Z","iopub.execute_input":"2025-12-26T17:35:29.058314Z","iopub.status.idle":"2025-12-26T17:35:29.071106Z","shell.execute_reply.started":"2025-12-26T17:35:29.058278Z","shell.execute_reply":"2025-12-26T17:35:29.070377Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"prompt = \"Dear boss ...\"\ntext_generated = simple_text_generation(prompt,\n                                        model,\n                                        tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:29.073485Z","iopub.execute_input":"2025-12-26T17:35:29.073771Z","iopub.status.idle":"2025-12-26T17:35:32.446657Z","shell.execute_reply.started":"2025-12-26T17:35:29.073748Z","shell.execute_reply":"2025-12-26T17:35:32.445985Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(text_generated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.447558Z","iopub.execute_input":"2025-12-26T17:35:32.447857Z","iopub.status.idle":"2025-12-26T17:35:32.451976Z","shell.execute_reply.started":"2025-12-26T17:35:32.447812Z","shell.execute_reply":"2025-12-26T17:35:32.451311Z"}},"outputs":[{"name":"stdout","text":"Dear boss ... I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I'm not going to be able to do this anymore. I\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.453009Z","iopub.execute_input":"2025-12-26T17:35:32.453388Z","iopub.status.idle":"2025-12-26T17:35:32.534183Z","shell.execute_reply.started":"2025-12-26T17:35:32.453351Z","shell.execute_reply":"2025-12-26T17:35:32.533435Z"}},"outputs":[{"name":"stdout","text":"GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"data = sentences = [\n    \"this is all about tokenization\",\n    \"Tokenization transforms raw text into structured units called tokens, enabling language models to process sentences numerically while preserving linguistic meaning through consistent mapping between text fragments and integer identifiers.\",\n\n    \"Embedding layers convert token identifiers into dense continuous vectors, allowing neural networks to learn semantic similarity by placing related words closer together in high dimensional vector space during training.\",\n\n    \"Subword tokenization techniques such as byte pair encoding help models represent rare or unseen words by decomposing them into smaller meaningful units that still receive informative embeddings.\",\n\n    \"A tokenizer defines vocabulary size and token boundaries, directly influencing memory usage, sequence length, and the quality of embeddings learned by transformer based language models.\",\n\n    \"Word embeddings are learned parameters that capture semantic relationships, enabling models to infer meaning, analogy, and contextual relevance rather than treating words as isolated symbols.\",\n\n    \"When text is tokenized, punctuation, whitespace, and special characters are handled explicitly so the resulting token stream remains consistent across different inputs and training environments.\",\n\n    \"Embedding vectors are typically initialized randomly and gradually optimized through gradient descent so that contextual patterns in language are reflected in their numerical representations.\",\n\n    \"Tokenization choices affect downstream performance because poorly designed token splits can fragment meaning and make it harder for embedding layers to capture semantic coherence.\",\n\n    \"In transformer models, each token embedding represents a combination of lexical meaning and learned structure before positional information is added to encode word order.\",\n\n    \"Context independent embeddings assign one vector per token, while contextual embeddings adjust representations dynamically based on surrounding tokens within a sentence.\",\n\n    \"Padding tokens are introduced during tokenization to align sequence lengths in a batch, and their embeddings are usually masked to avoid influencing model predictions.\",\n\n    \"Embedding dimensions control the expressive power of a model, with larger dimensions allowing richer representations at the cost of increased computation and memory usage.\",\n\n    \"Tokenizers map text to integers deterministically, ensuring reproducibility so the same input sentence always produces identical token sequences across experiments.\",\n\n    \"Shared embedding spaces enable models to compare tokens mathematically, allowing cosine similarity or dot product operations to reveal semantic closeness between words.\",\n\n    \"Special tokens such as start of sequence and end of sequence guide models during training by clearly marking sentence boundaries in the tokenized input.\",\n\n    \"Tokenization errors propagate forward, meaning poorly segmented text can limit the quality of embeddings no matter how powerful the downstream neural architecture is.\",\n\n    \"Embedding layers act as a lookup table where each row corresponds to a token vector that is continuously refined as the model learns from large text datasets.\",\n\n    \"Character level tokenization avoids unknown words but increases sequence length dramatically, making embedding learning more computationally expensive for long inputs.\",\n\n    \"Subword embeddings strike a balance between vocabulary size and semantic granularity, making them effective for multilingual and low resource language modeling tasks.\",\n\n    \"During inference, tokenized text is passed through frozen embedding layers that transform symbolic input into numerical form suitable for matrix operations.\",\n\n    \"Embedding similarity allows models to generalize, so words appearing in similar contexts produce related vectors even if they never appear together explicitly.\",\n\n    \"Tokenization schemes differ across models, meaning embeddings trained with one tokenizer are generally incompatible with models expecting another vocabulary.\",\n\n    \"Learned embeddings encode both syntactic and semantic information, allowing models to understand grammatical roles as well as conceptual meaning.\",\n\n    \"Byte level tokenization ensures every possible input can be represented, but often produces longer token sequences requiring careful embedding optimization.\",\n\n    \"Embedding matrices can be inspected directly in frameworks like PyTorch, revealing how tokens correspond to rows of trainable numerical parameters.\",\n\n    \"Tokenization converts unstructured text into a discrete representation that neural networks can efficiently batch, embed, and process in parallel.\",\n\n    \"Pretrained embeddings provide a strong initialization that helps models converge faster by starting from linguistically informed representations.\",\n\n    \"Token embeddings are shared across all occurrences of a token, allowing consistent meaning to be reinforced across many training examples.\",\n\n    \"Positional embeddings are added to token embeddings so models can distinguish between identical tokens appearing at different positions in a sequence.\",\n\n    \"The quality of embeddings depends heavily on data diversity, since richer corpora expose tokens to varied contexts that shape their vector representations.\",\n\n    \"Tokenization must balance linguistic accuracy with computational efficiency to avoid unnecessary fragmentation of common words.\",\n\n    \"Embedding vectors live in continuous space, enabling smooth interpolation between meanings rather than rigid categorical distinctions.\",\n\n    \"Tokenizers handle casing rules differently, meaning lowercasing text can significantly impact embedding reuse and vocabulary size.\",\n\n    \"In causal language models, token embeddings are optimized to predict the next token, reinforcing contextual relationships through training objectives.\",\n\n    \"Embedding lookup is one of the first operations in a language model forward pass, transforming integer inputs into floating point tensors.\",\n\n    \"Subword tokenization helps reduce out of vocabulary issues while allowing embeddings to capture meaningful morphological patterns.\",\n\n    \"Embedding layers are typically followed by attention mechanisms that refine token representations based on interactions with neighboring tokens.\",\n\n    \"Tokenization defines how text is segmented, but embeddings determine how those segments are understood numerically by the model.\",\n\n    \"Training embeddings jointly with the model allows them to adapt to task specific language usage rather than remaining static.\",\n\n    \"Token embeddings encode prior knowledge learned during pretraining, enabling downstream tasks to benefit from general language understanding.\",\n\n    \"Whitespace handling during tokenization affects how embeddings represent word boundaries and sentence structure.\",\n\n    \"Embedding normalization techniques can improve stability by keeping vector magnitudes within reasonable bounds.\",\n\n    \"Tokenizers must be deterministic so embedding lookup remains consistent across distributed training environments.\",\n\n    \"Embedding similarity can reveal biases present in training data, as tokens reflecting similar contexts cluster together.\",\n\n    \"Special tokens receive their own embeddings, allowing models to treat structural markers differently from regular text tokens.\",\n\n    \"Tokenization errors often appear subtle but can degrade embedding quality in long sequences.\",\n\n    \"Embedding matrices grow linearly with vocabulary size, making efficient tokenization essential for scaling large models.\",\n\n    \"Contextual embeddings evolve across transformer layers, refining token meaning as more context is incorporated.\",\n\n    \"Tokenization is language dependent, so multilingual models rely heavily on shared subword embeddings.\",\n\n    \"Embedding layers are differentiable components that learn through gradient updates during backpropagation.\",\n\n    \"Token frequency influences embedding quality, as rare tokens receive fewer updates during training.\",\n\n    \"Embedding inspection helps researchers understand how models internalize linguistic structure.\",\n\n    \"Tokenizers define how numbers, symbols, and punctuation are represented before embedding lookup.\",\n\n    \"Embedding vectors allow models to compute relationships using linear algebra rather than symbolic rules.\",\n\n    \"Tokenization choices influence sequence length, which directly impacts attention complexity.\",\n\n    \"Embedding layers compress discrete token identities into dense numerical forms suitable for neural computation.\",\n\n    \"Poor tokenization can increase sequence length unnecessarily, reducing embedding efficiency.\",\n\n    \"Embedding sharing between input and output layers reduces parameters and improves generalization.\",\n\n    \"Tokenization determines the granularity at which meaning is represented in embeddings.\",\n\n    \"Embedding spaces often capture analogical relationships such as semantic similarity or oppositeness.\",\n\n    \"Tokenization pipelines must remain consistent between training and inference to preserve embedding alignment.\",\n\n    \"Embedding learning benefits from large corpora where tokens appear in diverse linguistic contexts.\",\n\n    \"Tokenizers may split words differently depending on prefixes, suffixes, or frequency statistics.\",\n\n    \"Embedding vectors are updated incrementally as models learn from prediction errors.\",\n\n    \"Tokenization is a preprocessing step, but embeddings are learned representations within the model.\",\n\n    \"Embedding dimensionality reflects a tradeoff between expressiveness and computational cost.\",\n\n    \"Tokenization errors are difficult to correct after embedding lookup has occurred.\",\n\n    \"Embedding layers serve as the bridge between symbolic language and numerical computation.\",\n\n    \"Tokenizers encode language rules implicitly through their vocabulary construction process.\",\n\n    \"Embedding similarity metrics enable semantic search and clustering applications.\",\n\n    \"Tokenization defines model input structure, while embeddings define representational meaning.\",\n\n    \"Embedding matrices can be visualized to analyze semantic clustering of tokens.\",\n\n    \"Tokenization must handle edge cases like emojis, URLs, and code snippets consistently.\",\n\n    \"Embedding vectors evolve during training to reflect task specific linguistic patterns.\",\n\n    \"Tokenization consistency ensures embeddings remain meaningful across different datasets.\",\n\n    \"Embedding layers translate discrete token indices into continuous feature representations.\",\n\n    \"Tokenization granularity affects how efficiently embeddings encode meaning.\",\n\n    \"Embedding quality is tightly coupled with tokenizer design decisions.\",\n\n    \"Tokenization strategies influence how models generalize to unseen text.\",\n\n    \"Embedding learning enables neural models to capture language structure without explicit rules.\",\n\n    \"Tokenization converts language into a format embeddings can transform into meaning.\",\n\n    \"Embedding layers are foundational components underlying modern natural language processing systems.\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.535294Z","iopub.execute_input":"2025-12-26T17:35:32.535723Z","iopub.status.idle":"2025-12-26T17:35:32.551360Z","shell.execute_reply.started":"2025-12-26T17:35:32.535685Z","shell.execute_reply":"2025-12-26T17:35:32.550514Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# all inputs must have the same length\n# add a dummy token at the end\n# Having the same length => this is called padding\n\ntokenizer.pad_token = tokenizer.eos_token \n\ntokenized_data = [tokenizer.encode_plus(\n    sentence,\n    add_special_tokens= True,\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    max_length=50,\n    \n) for sentence in data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.552319Z","iopub.execute_input":"2025-12-26T17:35:32.552582Z","iopub.status.idle":"2025-12-26T17:35:32.589780Z","shell.execute_reply.started":"2025-12-26T17:35:32.552560Z","shell.execute_reply":"2025-12-26T17:35:32.589161Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenized_data[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.590622Z","iopub.execute_input":"2025-12-26T17:35:32.590897Z","iopub.status.idle":"2025-12-26T17:35:32.597940Z","shell.execute_reply.started":"2025-12-26T17:35:32.590860Z","shell.execute_reply":"2025-12-26T17:35:32.597287Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[{'input_ids': tensor([[ 5661,   318,   477,   546, 11241,  1634, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]])},\n {'input_ids': tensor([[30642,  1634, 31408,  8246,  2420,   656, 20793,  4991,  1444, 16326,\n             11, 15882,  3303,  4981,   284,  1429, 13439,  5470,  1146,   981,\n          23934, 29929,  3616,   832,  6414, 16855,  1022,  2420, 21441,   290,\n          18253, 42814,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]])}]"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"input_ids = [item[\"input_ids\"].squeeze() for item in tokenized_data]\nattention_masks = [mask[\"attention_mask\"].squeeze() for mask in tokenized_data]\ninput_ids[:5],attention_masks[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.598767Z","iopub.execute_input":"2025-12-26T17:35:32.599061Z","iopub.status.idle":"2025-12-26T17:35:32.615503Z","shell.execute_reply.started":"2025-12-26T17:35:32.599029Z","shell.execute_reply":"2025-12-26T17:35:32.614831Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"([tensor([ 5661,   318,   477,   546, 11241,  1634, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  tensor([30642,  1634, 31408,  8246,  2420,   656, 20793,  4991,  1444, 16326,\n             11, 15882,  3303,  4981,   284,  1429, 13439,  5470,  1146,   981,\n          23934, 29929,  3616,   832,  6414, 16855,  1022,  2420, 21441,   290,\n          18253, 42814,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  tensor([31567,  6048,   278, 11685, 10385, 11241, 42814,   656, 15715, 12948,\n          30104,    11,  5086, 17019,  7686,   284,  2193, 37865, 26789,   416,\n          12560,  3519,  2456,  5699,  1978,   287,  1029, 38517, 15879,  2272,\n           1141,  3047,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  tensor([ 7004,  4775, 11241,  1634,  7605,   884,   355, 18022,  5166, 21004,\n           1037,  4981,  2380,  4071,   393, 29587,  2456,   416, 26969, 32927,\n            606,   656,  4833, 11570,  4991,   326,   991,  3328, 30304, 11525,\n             67,   654,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  tensor([   32, 11241,  7509, 15738, 25818,  2546,   290, 11241, 13215,    11,\n           3264, 32596,  4088,  8748,    11,  8379,  4129,    11,   290,   262,\n           3081,   286, 11525,    67,   654,  4499,   416, 47385,  1912,  3303,\n           4981,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])],\n [tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0])])"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"### Convert the input_ids and attention mask to tensors","metadata":{}},{"cell_type":"code","source":"inputs_ids_tensor = torch.stack(input_ids)\nattention_masks_tensor = torch.stack(attention_masks)\ninputs_ids_tensor[:3],attention_masks_tensor[:3]\n\nprint(inputs_ids_tensor.shape)\n# print(input_ids.shape) # python list doesn't have shape property, that is why we change to torch tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.616396Z","iopub.execute_input":"2025-12-26T17:35:32.616699Z","iopub.status.idle":"2025-12-26T17:35:32.621848Z","shell.execute_reply.started":"2025-12-26T17:35:32.616667Z","shell.execute_reply":"2025-12-26T17:35:32.621111Z"}},"outputs":[{"name":"stdout","text":"torch.Size([83, 50])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"padded_input_ids = pad_sequence(inputs_ids_tensor,\n             batch_first=True,\n             padding_value=tokenizer.eos_token_id)\npadded_attention_masks = pad_sequence(attention_masks_tensor,\n                                     batch_first=True,\n                                     padding_value=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.622807Z","iopub.execute_input":"2025-12-26T17:35:32.623216Z","iopub.status.idle":"2025-12-26T17:35:32.637589Z","shell.execute_reply.started":"2025-12-26T17:35:32.623181Z","shell.execute_reply":"2025-12-26T17:35:32.636763Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"padded_input_ids[:2],padded_attention_masks[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.638650Z","iopub.execute_input":"2025-12-26T17:35:32.639059Z","iopub.status.idle":"2025-12-26T17:35:32.648484Z","shell.execute_reply.started":"2025-12-26T17:35:32.639015Z","shell.execute_reply":"2025-12-26T17:35:32.647641Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 5661,   318,   477,   546, 11241,  1634, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n         [30642,  1634, 31408,  8246,  2420,   656, 20793,  4991,  1444, 16326,\n             11, 15882,  3303,  4981,   284,  1429, 13439,  5470,  1146,   981,\n          23934, 29929,  3616,   832,  6414, 16855,  1022,  2420, 21441,   290,\n          18253, 42814,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]),\n tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]]))"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self,input_ids,attention_masks):\n        self.input_ids = input_ids\n        self.attention_masks = attention_masks\n        self.labels = input_ids.clone()\n    def __len__(self):\n        return len(self.input_ids)\n    def __getitem__(self,index):\n        return {\n            \"input_ids\":self.input_ids[index],\n            \"attention_mask\":self.attention_masks[index],\n            \"labels\":self.labels[index]\n        }\n\n\ndataset = TextDataset(inputs_ids_tensor,attention_masks_tensor)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.649289Z","iopub.execute_input":"2025-12-26T17:35:32.649548Z","iopub.status.idle":"2025-12-26T17:35:32.660554Z","shell.execute_reply.started":"2025-12-26T17:35:32.649515Z","shell.execute_reply":"2025-12-26T17:35:32.659797Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"len(dataset),dataset[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.661574Z","iopub.execute_input":"2025-12-26T17:35:32.661822Z","iopub.status.idle":"2025-12-26T17:35:32.676468Z","shell.execute_reply.started":"2025-12-26T17:35:32.661800Z","shell.execute_reply":"2025-12-26T17:35:32.675653Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(83,\n {'input_ids': tensor([31567,  6048,   278, 11685, 10385, 11241, 42814,   656, 15715, 12948,\n          30104,    11,  5086, 17019,  7686,   284,  2193, 37865, 26789,   416,\n          12560,  3519,  2456,  5699,  1978,   287,  1029, 38517, 15879,  2272,\n           1141,  3047,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0]),\n  'labels': tensor([31567,  6048,   278, 11685, 10385, 11241, 42814,   656, 15715, 12948,\n          30104,    11,  5086, 17019,  7686,   284,  2193, 37865, 26789,   416,\n          12560,  3519,  2456,  5699,  1978,   287,  1029, 38517, 15879,  2272,\n           1141,  3047,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])})"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### Fine tuning the GPT2 model\n","metadata":{}},{"cell_type":"code","source":"data_loader = DataLoader(dataset,batch_size=2,shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.677426Z","iopub.execute_input":"2025-12-26T17:35:32.677715Z","iopub.status.idle":"2025-12-26T17:35:32.686837Z","shell.execute_reply.started":"2025-12-26T17:35:32.677680Z","shell.execute_reply":"2025-12-26T17:35:32.686236Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# data_loader\n# for batch in data_loader:\n#     print(batch)\n#     print(\"\\n\"*5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.687824Z","iopub.execute_input":"2025-12-26T17:35:32.688048Z","iopub.status.idle":"2025-12-26T17:35:32.699944Z","shell.execute_reply.started":"2025-12-26T17:35:32.688026Z","shell.execute_reply":"2025-12-26T17:35:32.699320Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"model.parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.701057Z","iopub.execute_input":"2025-12-26T17:35:32.701374Z","iopub.status.idle":"2025-12-26T17:35:32.716500Z","shell.execute_reply.started":"2025-12-26T17:35:32.701338Z","shell.execute_reply":"2025-12-26T17:35:32.715743Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<bound method Module.parameters of GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(),lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.720738Z","iopub.execute_input":"2025-12-26T17:35:32.721101Z","iopub.status.idle":"2025-12-26T17:35:32.730651Z","shell.execute_reply.started":"2025-12-26T17:35:32.721075Z","shell.execute_reply":"2025-12-26T17:35:32.729903Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Set the model to training mode\nmodel.train()\n\n# Training loop\nfor epoch in range(10):\n    for batch in data_loader:\n        # Unpacking the input and atttention mask ids\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        # Reset the gradients to zero\n        optimizer.zero_grad() \n        #forward pass\n        outputs = model(input_ids=input_ids,\n                       attention_mask=attention_mask,\n                       labels=input_ids)\n        loss = outputs.loss\n        #backward pass\n        loss.backward()\n        #update the model parameters\n        optimizer.step()\n    # print the loss for the current epoch to monitor the progress\n    print(f\"Epoch {epoch+1} -Loss: {loss.item()}\")\n    \n        \n\n        \n\n\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T17:35:32.731642Z","iopub.execute_input":"2025-12-26T17:35:32.731866Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 -Loss: 1.5236197710037231\nEpoch 2 -Loss: 1.1036896705627441\nEpoch 3 -Loss: 1.5189203023910522\nEpoch 4 -Loss: 1.0782039165496826\nEpoch 5 -Loss: 0.7591965198516846\nEpoch 6 -Loss: 0.4072396457195282\nEpoch 7 -Loss: 0.6964302062988281\nEpoch 8 -Loss: 0.247667133808136\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Define funciton to generate text","metadata":{}},{"cell_type":"code","source":"def generate_text(prompt,model,tokenizer,max_length=100):\n    inputs = tokenizer.encode_plus(prompt,return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n    outputs = model.generate(input_ids,\n                             attention_mask=attention_mask,\n                             max_length=max_length)\n    return tokenizer.decode(outputs[0],skip_special_tokens=True)\n\nprompt = \"what is Embedding?\"\n\ntext_generated = generate_text(prompt,model,tokenizer,max_length=500)\nprint(f\"text_generated: {text_generated}\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenization and Embeddings","metadata":{}},{"cell_type":"code","source":"# install the faiss-cpu library\n!pip install faiss-cpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import faiss\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer,AutoModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the tokenizer and model for generating embeddings\nmodel_id = \"sentence-transformers/paraphrase-MiniLM-L6-V2\"\nembed_tokenizer = AutoTokenizer.from_pretrained(model_id)\nembed_model = AutoModel.from_pretrained(model_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"documents = [\n\n# ===================== EDUCATION (20) =====================\n\n\"Ethiopia’s rapid university expansion after 2010 produced dozens of public institutions, but many campuses faced shortages of qualified faculty, laboratory equipment, and digital libraries, leading to uneven educational quality between flagship universities and newer regional institutions.\",\n\n\"The introduction of modular curricula in Ethiopian universities aimed to improve practical skills, yet implementation varied widely due to limited industry partnerships and insufficient internship opportunities in smaller regional economies.\",\n\n\"Secondary education reforms emphasized national exam standardization, but disparities persisted as rural schools struggled with teacher turnover, language-of-instruction transitions, and limited access to preparatory materials.\",\n\n\"Ethiopia’s push to expand STEM education increased engineering enrollments, while insufficient computing infrastructure and outdated syllabi slowed alignment with modern software and AI industry requirements.\",\n\n\"The growth of private universities widened access to higher education, but raised concerns regarding accreditation consistency, faculty qualifications, and graduate employability in Ethiopia’s constrained labor market.\",\n\n\"Language policy in Ethiopian education required students to transition from regional languages to English instruction, creating comprehension gaps that disproportionately affected rural learners during secondary and tertiary education.\",\n\n\"Teacher training colleges expanded rapidly, yet continuous professional development remained limited, affecting pedagogical quality in overcrowded classrooms across fast-growing urban and peri-urban areas.\",\n\n\"The COVID-19 school closures accelerated digital learning experiments, revealing significant inequalities in device ownership, electricity access, and household learning environments across Ethiopian regions.\",\n\n\"Technical and vocational education reforms aimed to address youth unemployment, but societal preference for university degrees reduced enrollment in TVET programs despite labor demand.\",\n\n\"University research output increased modestly, though limited funding and heavy teaching loads constrained faculty engagement in internationally competitive research activities.\",\n\n\"Ethiopia’s school feeding programs improved attendance in food-insecure regions, but funding sustainability and supply chain reliability remained ongoing operational challenges.\",\n\n\"Postgraduate education expanded in Ethiopian universities, yet doctoral supervision capacity lagged behind enrollment growth, affecting completion timelines and research quality.\",\n\n\"Curriculum decentralization allowed regions to adapt content to local contexts, though uneven capacity resulted in inconsistent learning outcomes nationwide.\",\n\n\"Digital student information systems were introduced in some universities, but interoperability challenges limited nationwide academic data integration.\",\n\n\"Gender parity initiatives improved female enrollment, yet retention gaps persisted due to early marriage, household labor expectations, and safety concerns.\",\n\n\"National education roadmaps emphasized competency-based learning, though assessment practices often remained exam-centered due to institutional inertia.\",\n\n\"Ethiopia’s boarding school models aimed to serve pastoralist communities, balancing mobility challenges with formal education continuity.\",\n\n\"The expansion of community schools increased rural access, but infrastructure quality and teacher allocation remained uneven.\",\n\n\"Graduate unemployment influenced student migration toward perceived marketable fields, sometimes oversaturating specific disciplines.\",\n\n\"Education financing reforms debated cost-sharing models amid rising enrollment pressures and limited public budgets.\",\n\n\n# ===================== RELIGION / ORTHODOX (20) =====================\n\n\"The Ethiopian Orthodox Tewahedo Church remains deeply intertwined with national identity, yet internal administrative disputes increasingly intersected with ethnic and regional political dynamics.\",\n\n\"Orthodox church education systems preserved Ge’ez literacy traditions, though declining enrollment among youth raised concerns about intergenerational knowledge transmission.\",\n\n\"Recent disagreements over ecclesiastical jurisdiction reflected broader federal-regional tensions rather than purely theological differences.\",\n\n\"The church’s vast land holdings influenced urban development negotiations, especially in expanding cities like Addis Ababa and Bahir Dar.\",\n\n\"Religious festivals continued to structure communal calendars, even as urbanization altered participation patterns and ritual practices.\",\n\n\"Monasteries in remote regions played roles in environmental conservation through traditional land stewardship practices.\",\n\n\"Interfaith relations evolved as urban neighborhoods hosted Orthodox, Muslim, and Protestant communities in close proximity.\",\n\n\"Digital media enabled clergy and lay scholars to disseminate teachings, altering traditional authority channels within the church.\",\n\n\"The training of priests faced challenges due to economic pressures that diverted youth toward income-generating activities.\",\n\n\"Disputes over language use in liturgy mirrored broader debates about cultural representation and inclusion.\",\n\n\"Orthodox charitable organizations expanded social services during humanitarian crises, supplementing limited state capacity.\",\n\n\"Church music traditions adapted to modern recording technologies while preserving liturgical structures.\",\n\n\"The role of the Orthodox Church in mediation efforts varied across regional conflicts.\",\n\n\"Restoration projects of ancient churches relied increasingly on diaspora funding.\",\n\n\"Urban parish administration struggled with rapid population growth and resource constraints.\",\n\n\"The church’s calendar influenced agricultural labor cycles in rural communities.\",\n\n\"Heritage preservation debates emerged around modernization near historic religious sites.\",\n\n\"Religious education curricula balanced doctrinal instruction with contemporary social issues.\",\n\n\"The visibility of clergy in public discourse evolved with increased media exposure.\",\n\n\"Monastic tourism raised sustainability and preservation concerns.\",\n\n\n# ===================== TECHNOLOGY (20) =====================\n\n\"Ethiopia’s telecom liberalization introduced competition, reshaping data pricing, network expansion priorities, and mobile financial service adoption beyond major urban centers.\",\n\n\"Local software developers faced constraints from limited access to international payment systems, affecting participation in global digital marketplaces.\",\n\n\"Fintech growth expanded mobile payments, though interoperability challenges persisted between platforms.\",\n\n\"Startup hubs in Addis Ababa supported innovation but struggled with funding continuity.\",\n\n\"Digital ID rollout aimed to unify service access while raising privacy and infrastructure concerns.\",\n\n\"Ethiopia’s data center investments targeted government digitization needs.\",\n\n\"Internet shutdowns disrupted digital businesses and remote education efforts.\",\n\n\"Agri-tech pilots used satellite data for yield estimation.\",\n\n\"E-commerce adoption remained constrained by logistics and addressing systems.\",\n\n\"Open-source communities grew through university-linked tech clubs.\",\n\n\"Cloud adoption was limited by bandwidth costs.\",\n\n\"Ride-hailing apps navigated regulatory uncertainty.\",\n\n\"AI research groups emerged within universities despite compute limitations.\",\n\n\"Digital health platforms supported appointment scheduling pilots.\",\n\n\"Payment APIs expanded merchant digitalization.\",\n\n\"Drone technology was tested for land surveying.\",\n\n\"Tech policy debates focused on data localization.\",\n\n\"Digital literacy programs targeted youth employment.\",\n\n\"Smart meter projects aimed to reduce utility losses.\",\n\n\"Local language NLP research faced dataset scarcity.\",\n\n\n# ===================== AGRICULTURE (20) =====================\n\n\"Ethiopia’s wheat self-sufficiency drive expanded irrigated farming in lowland areas.\",\n\n\"Smallholder farmers adopted improved seed varieties unevenly.\",\n\n\"Extension services increasingly used mobile messaging.\",\n\n\"Coffee traceability reforms enabled specialty exports.\",\n\n\"Climate variability affected planting calendars.\",\n\n\"Pastoralist mobility conflicted with fixed land use policies.\",\n\n\"Post-harvest losses remained a major challenge.\",\n\n\"Irrigation schemes altered traditional water-sharing norms.\",\n\n\"Fertilizer distribution reforms affected timing and access.\",\n\n\"Livestock exports depended on disease control systems.\",\n\n\"Agro-processing parks aimed to increase value addition.\",\n\n\"Soil degradation influenced long-term productivity.\",\n\n\"Rainfall forecasting tools supported adaptive planning.\",\n\n\"Market access varied due to road infrastructure.\",\n\n\"Urban demand shaped peri-urban farming.\",\n\n\"Seed certification improved quality assurance.\",\n\n\"Cooperative governance affected farmer bargaining power.\",\n\n\"Mechanization adoption was limited by cost.\",\n\n\"Crop insurance pilots addressed climate risk.\",\n\n\"Export crop diversification strategies expanded.\",\n\n\n# ===================== POLITICS (20) =====================\n\n\"Ethiopia’s federal system faced governance challenges amid shifting regional power dynamics.\",\n\n\"Electoral processes were influenced by security conditions.\",\n\n\"Decentralization policies complicated service delivery coordination.\",\n\n\"Political party fragmentation affected coalition stability.\",\n\n\"Administrative boundary disputes impacted local governance.\",\n\n\"Security sector reforms intersected with regional authority.\",\n\n\"Media liberalization expanded public discourse.\",\n\n\"Emergency regulations affected civil liberties debates.\",\n\n\"Peace negotiations shaped post-conflict transitions.\",\n\n\"Federal–regional fiscal relations influenced budget allocation.\",\n\n\"Identity politics shaped voter mobilization strategies.\",\n\n\"Judicial reform aimed to strengthen independence.\",\n\n\"Public sector reform targeted efficiency improvements.\",\n\n\"Foreign policy balanced regional diplomacy priorities.\",\n\n\"Legislative capacity faced resource constraints.\",\n\n\"Civil society space evolved under new regulations.\",\n\n\"Conflict resolution mechanisms varied by region.\",\n\n\"Urban governance reforms addressed service delivery.\",\n\n\"Policy continuity challenges followed leadership transitions.\",\n\n\"Political dialogue platforms aimed to reduce polarization.\"\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def embed_text(text,tokenizer,model):\n    inputs = tokenizer(text,\n                       return_tensors=\"pt\",\n                       padding=True,\n                       truncation=True\n                       \n                      )\n    with torch.no_grad(): # tell pytorch to perform inference and don't build computation graph for backpass and gradient\n        embeddings = model(**inputs).last_hidden_state # last context riched tokens\n        embeddings = embeddings.mean(dim=1) # pooling token embeddings into single sentence embedding for the seek of retrieval system\n    return embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embed_model,embed_tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize a list to store the bembeddings\ndocument_embeddings = []\n\nfor doc in documents:\n    doc_embeddings = embed_text(doc,embed_tokenizer,embed_model)\n    document_embeddings.append(doc_embeddings)\n\n\ndocument_embeddings[1].shape\n ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" document_embeddings = torch.cat(document_embeddings).cpu().numpy()\ndocument_embeddings.shape, document_embeddings[:2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Build the Retrieval System","metadata":{}},{"cell_type":"code","source":"document_embeddings.shape[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" index = faiss.IndexFlatL2(document_embeddings.shape[1])\nindex.add(document_embeddings)\nprint(index)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"faiss.downcast_index(index)\nindex.d, index.ntotal, index.is_trained,index.metric_type","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieval -> Build a function to retrieve information\ndef retrieve(query,tokenizer,model,index,documents,top_k=3):\n    query_embeddings = embed_text(query,tokenizer,model)\n    distances, indices = index.search(query_embeddings,top_k)\n    return [documents[i] for i in indices[0]],distances[0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the retrieval function\nquery = \"ethiopain technology development journey\"\nretrieved_docs,distances = retrieve(query,embed_tokenizer,embed_model,index,documents)\nfor d in retrieved_docs:\n    print(d)\n    print(\"\\n\"*5)\nprint(distances)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Integrating The Generative System","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM,AutoTokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the generative tokenizer and model\ngen_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ngen_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ngen_tokenizer.pad_token = gen_tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context = \" \".join(retrieved_docs)\ncontext","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to generate context riched text\ndef generate_text(context,query,model,tokenizer):\n    input_text =   f\"Context: {context} \\n Question:{query} \\n Answer:\"\n    inputs = tokenizer(input_text,\n                       return_tensors=\"pt\",\n                       padding=True,\n                      truncation=True)\n    inputs_ids = inputs[\"input_ids\"]\n    (inputs_ids != tokenizer.pad_token_id)\n    attention_masks = inputs[\"attention_mask\"]\n    outputs = model.generate(inputs_ids,\n                                attention_mask=attention_masks,\n                                max_new_tokens=150,\n                                 do_sample=True,\n                                 top_p=80,\n                                 top_k=50,\n                                 repetition_penalty=1.2,\n                                 temperature=0.1,\n                             \n                             \n                                pad_token_id=gen_tokenizer.eos_token_id)\n    outputs.shape,outputs\n    \n    \n    return tokenizer.decode(outputs[0],skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generated_answer = generate_text(context,query,gen_model,gen_tokenizer)\nprint(f\"generated anser:\\n {generated_answer}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### RAG System","metadata":{}},{"cell_type":"code","source":"# Define the RAG function that integrates retrieval and generation\ndef rag(query,retrieval_tokenizer,retrieval_model,retrieval_index,gen_model,gen_tokenizer,documents,top_k=3):\n    retrieved_docs, distances = retrieve(query,\n                                      retrieval_tokenizer,\n                                      retrieval_model,\n                                      retrieval_index,\n                                      documents,\n                                         top_k)\n    context = \" \".join(retrieved_docs)\n    generated_answer = generate_text(context,query,gen_model,gen_tokenizer)\n    return generated_answer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the rag system\nquery = \"Ethiopian university learning strategy\"\nanswer = rag(query,embed_tokenizer,embed_model,index,gen_model,gen_tokenizer,documents)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}